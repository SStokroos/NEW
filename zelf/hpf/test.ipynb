{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Hierarchical Poisson Factorization\n",
      "**********************************\n",
      "\n",
      "Number of users: 100\n",
      "Number of items: 100\n",
      "Latent factors to use: 30\n",
      "\n",
      "Initializing parameters...\n",
      "Allocating Phi matrix...\n",
      "Initializing optimization procedure...\n",
      "Iteration 10 | train llk: -9872 | train rmse: 1.0955\n",
      "Iteration 20 | train llk: -9180 | train rmse: 1.0184\n",
      "Iteration 30 | train llk: -8297 | train rmse: 0.9146\n",
      "Iteration 40 | train llk: -8342 | train rmse: 0.9285\n",
      "\n",
      "\n",
      "Optimization finished\n",
      "Final log-likelihood: -8342\n",
      "Final RMSE: 0.9285\n",
      "Minutes taken (optimization part): 0.0\n",
      "\n",
      "**********************************\n",
      "Hierarchical Poisson Factorization\n",
      "**********************************\n",
      "\n",
      "Number of users: 100\n",
      "Number of items: 100\n",
      "Latent factors to use: 30\n",
      "\n",
      "Initializing parameters...\n",
      "Allocating Phi matrix...\n",
      "Initializing optimization procedure...\n",
      "Iteration 10 | val llk: -99 | val rmse: 1.0556\n",
      "Iteration 20 | val llk: -91 | val rmse: 0.9649\n",
      "Iteration 30 | val llk: -90 | val rmse: 0.8848\n",
      "Iteration 40 | val llk: -91 | val rmse: 0.8972\n",
      "\n",
      "\n",
      "Optimization finished\n",
      "Final log-likelihood: -91\n",
      "Final RMSE: 0.8972\n",
      "Minutes taken (optimization part): 0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd, numpy as np\n",
    "# from hpfrec import HPF\n",
    "\n",
    "# ## Generating sample counts data\n",
    "# nusers = 15400\n",
    "# nitems = 1000\n",
    "# nobs   = \n",
    "\n",
    "# np.random.seed(1)\n",
    "# counts_df = pd.DataFrame({\n",
    "# \t'UserId' : np.random.randint(nusers, size=nobs),\n",
    "# \t'ItemId' : np.random.randint(nitems, size=nobs),\n",
    "# \t'Count' :  (np.random.gamma(1,1, size=nobs) + 1).astype('int32')\n",
    "# \t})\n",
    "# counts_df = counts_df.loc[~counts_df[['UserId', 'ItemId']].duplicated()].reset_index(drop=True)\n",
    "\n",
    "# ## Initializing the model object\n",
    "# recommender = HPF()\n",
    "\n",
    "# ## For stochastic variational inference, need to select batch size (number of users)\n",
    "# recommender = HPF(users_per_batch = 20)\n",
    "\n",
    "# ## Full function call\n",
    "# recommender = HPF(\n",
    "# \tk=30, a=0.3, a_prime=0.3, b_prime=1.0,\n",
    "# \tc=0.3, c_prime=0.3, d_prime=1.0, ncores=-1,\n",
    "# \tstop_crit='train-llk', check_every=10, stop_thr=1e-3,\n",
    "# \tusers_per_batch=None, items_per_batch=None, step_size=lambda x: 1/np.sqrt(x+2),\n",
    "# \tmaxiter=100, use_float=True, reindex=True, verbose=True,\n",
    "# \trandom_seed=None, allow_inconsistent_math=False, full_llk=False,\n",
    "# \talloc_full_phi=False, keep_data=True, save_folder=None,\n",
    "# \tproduce_dicts=True, keep_all_objs=True, sum_exp_trick=False\n",
    "# )\n",
    "\n",
    "# ## Fitting the model to the data\n",
    "# recommender.fit(counts_df)\n",
    "\n",
    "# ## Fitting the model while monitoring a validation set\n",
    "# recommender = HPF(stop_crit='val-llk')\n",
    "# recommender.fit(counts_df, val_set=counts_df.sample(10**2))\n",
    "# ## Note: a real validation should NEVER be a subset of the training set\n",
    "\n",
    "# ## Fitting the model to data in batches passed by the user\n",
    "# recommender = HPF(reindex=False, keep_data=False)\n",
    "# users_batch1 = np.unique(np.random.randint(10**2, size=20))\n",
    "# users_batch2 = np.unique(np.random.randint(10**2, size=20))\n",
    "# users_batch3 = np.unique(np.random.randint(10**2, size=20))\n",
    "# recommender.partial_fit(counts_df.loc[counts_df.UserId.isin(users_batch1)], nusers=10**2, nitems=10**2)\n",
    "# recommender.partial_fit(counts_df.loc[counts_df.UserId.isin(users_batch2)])\n",
    "# recommender.partial_fit(counts_df.loc[counts_df.UserId.isin(users_batch3)])\n",
    "\n",
    "# ## Making predictions\n",
    "# # recommender.topN(user=10, n=10, exclude_seen=True) ## not available when using 'partial_fit'\n",
    "# recommender.topN(user=10, n=10, exclude_seen=False, items_pool=np.array([1,2,3,4]))\n",
    "# recommender.predict(user=10, item=11)\n",
    "# recommender.predict(user=[10,10,10], item=[1,2,3])\n",
    "# recommender.predict(user=[10,11,12], item=[4,5,6])\n",
    "\n",
    "# ## Evaluating Poisson likelihood\n",
    "# recommender.eval_llk(counts_df, full_llk=True)\n",
    "\n",
    "# ## Determining latent factors for a new user, given her item interactions\n",
    "# nobs_new = 20\n",
    "# np.random.seed(2)\n",
    "# counts_df_new = pd.DataFrame({\n",
    "# \t'ItemId' : np.random.choice(np.arange(nitems), size=nobs_new, replace=False),\n",
    "# \t'Count' : np.random.gamma(1,1, size=nobs_new).astype('int32')\n",
    "# \t})\n",
    "# counts_df_new = counts_df_new.loc[counts_df_new.Count > 0].reset_index(drop=True)\n",
    "# recommender.predict_factors(counts_df_new)\n",
    "\n",
    "# ## Adding a user without refitting the whole model\n",
    "# recommender.add_user(user_id=nusers+1, counts_df=counts_df_new)\n",
    "\n",
    "# ## Updating data for an existing user without refitting the whole model\n",
    "# chosen_user = counts_df.UserId.values[10]\n",
    "# recommender.add_user(user_id=chosen_user, counts_df=counts_df_new, update_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Hierarchical Poisson Factorization\n",
      "**********************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\ana\\Lib\\site-packages\\hpfrec\\__init__.py:469: UserWarning: 'counts_df' contains observations with a count value less than 1, these will be ignored. Any user or item associated exclusively with zero-value observations will be excluded. If using 'reindex=False', make sure that your data still meets the necessary criteria. If you still want to use these observations, set 'stop_crit' to 'diff-norm' or 'maxiter'.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Public\\ana\\Lib\\site-packages\\hpfrec\\__init__.py:478: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.input_df[\"UserId\"], self.user_mapping_ = pd.factorize(self.input_df[\"UserId\"])\n",
      "c:\\Users\\Public\\ana\\Lib\\site-packages\\hpfrec\\__init__.py:479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.input_df[\"ItemId\"], self.item_mapping_ = pd.factorize(self.input_df[\"ItemId\"])\n",
      "c:\\Users\\Public\\ana\\Lib\\site-packages\\hpfrec\\__init__.py:510: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.input_df['Count'] = self.input_df[\"Count\"].astype(cython_loops.c_real_t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 15400\n",
      "Number of items: 1000\n",
      "Latent factors to use: 30\n",
      "\n",
      "Initializing parameters...\n",
      "Allocating Phi matrix...\n",
      "Initializing optimization procedure...\n",
      "Iteration 10 | train llk: -1234780 | train rmse: 0.9076\n",
      "Iteration 20 | train llk: -1044695 | train rmse: 0.8477\n",
      "Iteration 30 | train llk: -1031548 | train rmse: 0.8439\n",
      "Iteration 40 | train llk: -1028453 | train rmse: 0.8432\n",
      "Iteration 50 | train llk: -1026587 | train rmse: 0.8428\n",
      "Iteration 60 | train llk: -1025480 | train rmse: 0.8427\n",
      "Iteration 70 | train llk: -1024768 | train rmse: 0.8427\n",
      "\n",
      "\n",
      "Optimization finished\n",
      "Final log-likelihood: -1024768\n",
      "Final RMSE: 0.8427\n",
      "Minutes taken (optimization part): 0.2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hpfrec.HPF at 0x1b19594d010>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from hpfrec import HPF\n",
    "\n",
    "train_data = pd.read_csv('C:\\\\Users\\\\Sten Stokroos\\\\Desktop\\\\zelf\\\\data\\\\Webscope_R3\\\\ydata-ymusic-rating-study-v1_0-train.txt', sep='\\t', header=None, names=['UserId', 'ItemId', 'Rating'])\n",
    "\n",
    "# We don't care about the actual rating value, only if there's been an interaction or not.\n",
    "# Therefore, we set exposure to 1 for all observed user-item pairs.\n",
    "train_data['Count'] = 1\n",
    "\n",
    "# Generate the full user-item matrix and fill in the missing values with 0 (no exposure).\n",
    "user_item_matrix = train_data.pivot_table(index='UserId', columns='ItemId', values='Count', fill_value=0)\n",
    "\n",
    "# Now we'll convert the user-item matrix back into the triplet format expected by HPFrec.\n",
    "# (Note: This step can be memory-intensive and might not be suitable for very large datasets)\n",
    "user_item_triplets = user_item_matrix.stack().reset_index(name='Count')\n",
    "\n",
    "# Initialize the HPF model with specified hyperparameters\n",
    "recommender = HPF(\n",
    "    k=30, a=0.3, a_prime=0.3, b_prime=0.3,\n",
    "    c=0.3, c_prime=0.3, d_prime=0.3, ncores=-1,\n",
    "    stop_crit='train-llk', check_every=10, stop_thr=1e-3,\n",
    "    maxiter=100, use_float=True, reindex=True, verbose=True,\n",
    "    random_seed=None, allow_inconsistent_math=False, full_llk=False,\n",
    "    alloc_full_phi=False, keep_data=True, save_folder=None,\n",
    "    produce_dicts=True, keep_all_objs=True, sum_exp_trick=False\n",
    ")\n",
    "\n",
    "# Fit the HPF model to the binary exposure data\n",
    "recommender.fit(user_item_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted exposures (substitute confounder) for all user-item pairs\n",
    "predicted_exposures = recommender.predict(user_item_triplets['UserId'], user_item_triplets['ItemId'])\n",
    "\n",
    "# The result is an array of predicted exposure rates for each user-item pair.\n",
    "# You can add this back to the dataframe if you want to analyze it further.\n",
    "user_item_triplets['PredictedExposure'] = predicted_exposures\n",
    "\n",
    "# If you want to create a full matrix again, you can pivot this data.\n",
    "# Note that this will create a dense matrix, which may be memory-intensive.\n",
    "full_predicted_matrix = user_item_triplets.pivot(index='UserId', columns='ItemId', values='PredictedExposure')\n",
    "\n",
    "# Now you have a full matrix of predicted exposures for each user-item pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_predicted_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m full_predicted_matrix\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m13\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming `user_item_triplets` has a column 'PredictedExposure' with the predicted values\u001b[39;00m\n\u001b[0;32m      4\u001b[0m top_25_exposures \u001b[38;5;241m=\u001b[39m user_item_triplets\u001b[38;5;241m.\u001b[39mnsmallest(\u001b[38;5;241m25\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredictedExposure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_predicted_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "full_predicted_matrix.iloc[0,13]\n",
    "\n",
    "# Assuming `user_item_triplets` has a column 'PredictedExposure' with the predicted values\n",
    "top_25_exposures = user_item_triplets.nsmallest(25, 'PredictedExposure')\n",
    "\n",
    "# Display the top 25 highest predicted exposures\n",
    "print(top_25_exposures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import os\n",
    "\n",
    "dir_r3 = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/out'\n",
    "dir_ml = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/out'\n",
    "randseed = 42\n",
    "\n",
    "def choose_data(dat, test_size=0.1):\n",
    "    if dat == 'r3':\n",
    "        train = pd.read_csv(os.path.join(dir_r3, 'r3_train.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "        test = pd.read_csv(os.path.join(dir_r3, 'r3_test.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "        \n",
    "        # Combine train and test to create the full dataset\n",
    "        r3_full = pd.concat([train, test]).sort_values(by=['userId', 'songId']).reset_index(drop=True)\n",
    "        \n",
    "        return r3_full, train, test\n",
    "    elif dat == 'ml':\n",
    "        ml_full = pd.read_csv(os.path.join(dir_ml, 'ml-1m_full.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "        train, test = train_test_split(ml_full, test_size=test_size, random_state=randseed)\n",
    "        return ml_full, train, test\n",
    "    else:\n",
    "        print('Wrong data input')\n",
    "        return None, None, None\n",
    "\n",
    "class UAutoRec():\n",
    "    def __init__(self, sess, num_user, num_item, learning_rate=0.001, reg_rate=0.1, epoch=20, batch_size=200,\n",
    "                 verbose=False, T=3, display_step=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.reg_rate = reg_rate\n",
    "        self.sess = sess\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.verbose = verbose\n",
    "        self.T = T\n",
    "        self.display_step = display_step\n",
    "        print(\"UAutoRec with Confounder.\")\n",
    "\n",
    "    def build_network(self, hidden_neuron=500):\n",
    "        self.rating_matrix = tf.compat.v1.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n",
    "        self.rating_matrix_mask = tf.compat.v1.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n",
    "        self.confounder_matrix = tf.compat.v1.placeholder(dtype=tf.float32, shape=[self.num_item, None])\n",
    "\n",
    "        # Rating path\n",
    "        V_R = tf.Variable(tf.random.normal([hidden_neuron, self.num_item], stddev=0.01))\n",
    "        mu_R = tf.Variable(tf.random.normal([hidden_neuron], stddev=0.01))\n",
    "        layer_1_R = tf.sigmoid(tf.expand_dims(mu_R, 1) + tf.matmul(V_R, self.rating_matrix))\n",
    "        \n",
    "        # Confounder path\n",
    "        V_C = tf.Variable(tf.random.normal([hidden_neuron, self.num_item], stddev=0.01))\n",
    "        mu_C = tf.Variable(tf.random.normal([hidden_neuron], stddev=0.01))\n",
    "        layer_1_C = tf.sigmoid(tf.expand_dims(mu_C, 1) + tf.matmul(V_C, self.confounder_matrix))\n",
    "        \n",
    "        # Combine paths\n",
    "        layer_1 = layer_1_R + layer_1_C\n",
    "        \n",
    "        # Output layer\n",
    "        W = tf.Variable(tf.random.normal([self.num_item, hidden_neuron], stddev=0.01))\n",
    "        b = tf.Variable(tf.random.normal([self.num_item], stddev=0.01))\n",
    "        self.layer_2 = tf.matmul(W, layer_1) + tf.expand_dims(b, 1)\n",
    "        self.loss = tf.reduce_mean(tf.square(\n",
    "            tf.norm(tf.multiply((self.rating_matrix - self.layer_2), self.rating_matrix_mask)))) + self.reg_rate * (\n",
    "        tf.square(tf.norm(W)) + tf.square(tf.norm(V_R)) + tf.square(tf.norm(V_C)))\n",
    "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "\n",
    "    def train(self, train_data, confounder_data):\n",
    "        self.num_training = self.num_user\n",
    "        total_batch = int(self.num_training / self.batch_size)\n",
    "        idxs = np.random.permutation(self.num_training)  # shuffled ordering\n",
    "\n",
    "        total_loss = 0\n",
    "        for i in range(total_batch):\n",
    "            start_time = time.time()\n",
    "            if i == total_batch - 1:\n",
    "                batch_set_idx = idxs[i * self.batch_size:]\n",
    "            elif i < total_batch - 1:\n",
    "                batch_set_idx = idxs[i * self.batch_size: (i + 1) * self.batch_size]\n",
    "\n",
    "            try:\n",
    "                _, loss = self.sess.run([self.optimizer, self.loss],\n",
    "                                        feed_dict={self.rating_matrix: self.train_data[:, batch_set_idx],\n",
    "                                                   self.rating_matrix_mask: self.train_data_mask[:, batch_set_idx],\n",
    "                                                   self.confounder_matrix: confounder_data[:, batch_set_idx]})\n",
    "                total_loss += loss\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError: {e}\")\n",
    "                print(f\"Max index in batch_set_idx: {max(batch_set_idx)}\")\n",
    "                print(f\"Train data shape: {self.train_data.shape}\")\n",
    "                print(f\"Confounder data shape: {confounder_data.shape}\")\n",
    "                raise\n",
    "\n",
    "        return total_loss / total_batch\n",
    "\n",
    "    def test(self, test_data, confounder_data):\n",
    "        self.reconstruction = self.sess.run(self.layer_2, feed_dict={self.rating_matrix: self.train_data,\n",
    "                                                                     self.rating_matrix_mask: self.train_data_mask,\n",
    "                                                                     self.confounder_matrix: confounder_data})\n",
    "        error = 0\n",
    "        error_mae = 0\n",
    "        test_set = list(test_data.keys())\n",
    "        for (u, i) in test_set:\n",
    "            pred_rating_test = self.predict(u, i)\n",
    "            error += (float(test_data.get((u, i))) - pred_rating_test) ** 2\n",
    "            error_mae += (np.abs(float(test_data.get((u, i))) - pred_rating_test))\n",
    "        rmse = RMSE(error, len(test_set))\n",
    "        mae = MAE(error_mae, len(test_set))\n",
    "        return rmse, mae\n",
    "\n",
    "    def execute(self, train_data, test_data, confounder_data):\n",
    "        self.train_data = self._data_process(train_data.transpose())\n",
    "        self.train_data_mask = np.sign(self.train_data)\n",
    "        print(f\"Train data processed shape: {self.train_data.shape}\")\n",
    "        print(f\"Confounder data shape: {confounder_data.shape}\")\n",
    "        init = tf.compat.v1.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "        with tqdm(total=self.epochs, desc=\"Training\", unit=\"epoch\") as pbar:\n",
    "            for epoch in range(self.epochs):\n",
    "                avg_loss = self.train(train_data, confounder_data)\n",
    "                if (epoch) % self.T == 0:\n",
    "                    rmse, mae = self.test(test_data, confounder_data)\n",
    "                    pbar.set_postfix({\"Loss\": avg_loss, \"RMSE\": rmse, \"MAE\": mae})\n",
    "                pbar.update(1)\n",
    "\n",
    "    def save(self, path):\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        saver.save(self.sess, path)\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        if user_id >= self.num_user or item_id >= self.num_item:\n",
    "            raise IndexError(\"user_id or item_id out of bounds\")\n",
    "        return self.reconstruction[item_id, user_id]\n",
    "\n",
    "    def _data_process(self, data):\n",
    "        output = np.zeros((self.num_item, self.num_user))\n",
    "        for u in range(self.num_user):\n",
    "            for i in range(self.num_item):\n",
    "                output[i, u] = data.get((i, u), 0)  # Use .get() with a default value of 0\n",
    "        return output\n",
    "\n",
    "def RMSE(error, num):\n",
    "    return np.sqrt(error / num)\n",
    "\n",
    "def MAE(error_mae, num):\n",
    "    return (error_mae / num)\n",
    "\n",
    "def load_data_rating(dat, columns=[0, 1, 2], sep=\"\\t\"):\n",
    "    full, train, test = choose_data(dat, test_size= 0.1)\n",
    "\n",
    "    \n",
    "    train, vad =  train_test_split(train, test_size=0.1, random_state=42)#pd.read_csv(train_file, sep=sep, header=None, names=['userId', 'itemId', 'rating'], usecols=columns, engine=\"python\")\n",
    "    \n",
    "    n_users = max(train['userId'].max(), test['userId'].max()) + 1\n",
    "    n_items = max(train['songId'].max(), test['songId'].max()) + 1\n",
    "\n",
    "    train_row = []\n",
    "    train_col = []\n",
    "    train_rating = []\n",
    "\n",
    "    for line in train.itertuples():\n",
    "        u = line[1]\n",
    "        i = line[2]\n",
    "        train_row.append(u)\n",
    "        train_col.append(i)\n",
    "        train_rating.append(line[3])\n",
    "\n",
    "    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n",
    "\n",
    "    test_row = []\n",
    "    test_col = []\n",
    "    test_rating = []\n",
    "    for line in test.itertuples():\n",
    "        u = line[1]\n",
    "        i = line[2]\n",
    "        test_row.append(u)\n",
    "        test_col.append(i)\n",
    "        test_rating.append(line[3])\n",
    "\n",
    "    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n",
    "\n",
    "    vd_row = []\n",
    "    vd_col = []\n",
    "    vd_rating = []\n",
    "    for line in vad.itertuples():\n",
    "        u = line[1]\n",
    "        i = line[2]\n",
    "        vd_row.append(u)\n",
    "        vd_col.append(i)\n",
    "        vd_rating.append(line[3])\n",
    "\n",
    "    vd_matrix = csr_matrix((vd_rating, (vd_row,vd_col)), shape=(n_users, n_items))\n",
    "\n",
    "    print(\"Load data finished. Number of users:\", n_users, \"Number of items:\", n_items)\n",
    "    return train_matrix.todok(), test_matrix.todok(), vd_matrix.todok(), n_users, n_items\n",
    "\n",
    "\n",
    "# train, test, vad, user, item = load_data_rating('r3', columns=[0, 1, 2], sep=\"\\t\")\n",
    "\n",
    "CAUSEFIT_DIR = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/exposure_output/ml_exp_k_30_050.csv'\n",
    "\n",
    "conf_df = pd.read_csv(CAUSEFIT_DIR, header=None)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "confounder_data = conf_df.to_numpy()\n",
    "confounder_data = confounder_data.T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAutoRec with Confounder.                            \n",
      "Train data processed shape: (3706, 6040)             \n",
      "Confounder data shape: (3706, 6040)                  \n",
      "  0%|          | 0/5 [01:59<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:06<?, ?epoch/s, Loss=1.8e+5, RMSE=1.37, MAE=1.07]\n",
      "Training:   5%|5         | 1/20 [00:06<02:10,  6.89s/epoch, Loss=1.8e+5, RMSE=1.37, MAE=1.07]\n",
      "Training:  10%|#         | 2/20 [00:10<01:24,  4.72s/epoch, Loss=1.8e+5, RMSE=1.37, MAE=1.07]\n",
      "Training:  15%|#5        | 3/20 [00:13<01:08,  4.02s/epoch, Loss=1.8e+5, RMSE=1.37, MAE=1.07]\n",
      "Training:  15%|#5        | 3/20 [00:19<01:08,  4.02s/epoch, Loss=5.68e+4, RMSE=1.02, MAE=0.817]\n",
      "Training:  20%|##        | 4/20 [00:19<01:19,  4.98s/epoch, Loss=5.68e+4, RMSE=1.02, MAE=0.817]\n",
      "Training:  25%|##5       | 5/20 [00:22<01:05,  4.34s/epoch, Loss=5.68e+4, RMSE=1.02, MAE=0.817]\n",
      "Training:  30%|###       | 6/20 [00:26<00:55,  3.96s/epoch, Loss=5.68e+4, RMSE=1.02, MAE=0.817]\n",
      "Training:  30%|###       | 6/20 [00:33<00:55,  3.96s/epoch, Loss=5.39e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  35%|###5      | 7/20 [00:33<01:06,  5.14s/epoch, Loss=5.39e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  40%|####      | 8/20 [00:37<00:55,  4.63s/epoch, Loss=5.39e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  45%|####5     | 9/20 [00:40<00:47,  4.30s/epoch, Loss=5.39e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  45%|####5     | 9/20 [00:46<00:47,  4.30s/epoch, Loss=5.1e+4, RMSE=0.977, MAE=0.774]\n",
      "Training:  50%|#####     | 10/20 [00:46<00:47,  4.77s/epoch, Loss=5.1e+4, RMSE=0.977, MAE=0.774]\n",
      "Training:  55%|#####5    | 11/20 [00:49<00:38,  4.25s/epoch, Loss=5.1e+4, RMSE=0.977, MAE=0.774]\n",
      "Training:  60%|######    | 12/20 [00:52<00:31,  3.89s/epoch, Loss=5.1e+4, RMSE=0.977, MAE=0.774]\n",
      "Training:  60%|######    | 12/20 [00:58<00:31,  3.89s/epoch, Loss=4.66e+4, RMSE=0.944, MAE=0.75]\n",
      "Training:  65%|######5   | 13/20 [00:58<00:31,  4.50s/epoch, Loss=4.66e+4, RMSE=0.944, MAE=0.75]\n",
      "Training:  70%|#######   | 14/20 [01:01<00:24,  4.08s/epoch, Loss=4.66e+4, RMSE=0.944, MAE=0.75]\n",
      "Training:  75%|#######5  | 15/20 [01:04<00:18,  3.78s/epoch, Loss=4.66e+4, RMSE=0.944, MAE=0.75]\n",
      "Training:  75%|#######5  | 15/20 [01:11<00:18,  3.78s/epoch, Loss=4.19e+4, RMSE=0.916, MAE=0.726]\n",
      "Training:  80%|########  | 16/20 [01:11<00:18,  4.54s/epoch, Loss=4.19e+4, RMSE=0.916, MAE=0.726]\n",
      "Training:  85%|########5 | 17/20 [01:14<00:12,  4.12s/epoch, Loss=4.19e+4, RMSE=0.916, MAE=0.726]\n",
      "Training:  90%|######### | 18/20 [01:17<00:07,  3.78s/epoch, Loss=4.19e+4, RMSE=0.916, MAE=0.726]\n",
      "Training:  90%|######### | 18/20 [01:23<00:07,  3.78s/epoch, Loss=3.8e+4, RMSE=0.902, MAE=0.715] \n",
      "Training:  95%|#########5| 19/20 [01:23<00:04,  4.41s/epoch, Loss=3.8e+4, RMSE=0.902, MAE=0.715]\n",
      "Training: 100%|##########| 20/20 [01:26<00:00,  4.04s/epoch, Loss=3.8e+4, RMSE=0.902, MAE=0.715]\n",
      "Training: 100%|##########| 20/20 [01:26<00:00,  4.32s/epoch, Loss=3.8e+4, RMSE=0.902, MAE=0.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 20%|██        | 1/5 [05:29<13:57, 209.43s/trial, best loss: 0.8983660924581586]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:06<?, ?epoch/s, Loss=2.51e+4, RMSE=1.07, MAE=0.842]\n",
      "Training:   5%|5         | 1/20 [00:06<01:54,  6.04s/epoch, Loss=2.51e+4, RMSE=1.07, MAE=0.842]\n",
      "Training:  10%|#         | 2/20 [00:09<01:23,  4.66s/epoch, Loss=2.51e+4, RMSE=1.07, MAE=0.842]\n",
      "Training:  15%|#5        | 3/20 [00:13<01:12,  4.26s/epoch, Loss=2.51e+4, RMSE=1.07, MAE=0.842]\n",
      "Training:  15%|#5        | 3/20 [00:19<01:12,  4.26s/epoch, Loss=1.39e+4, RMSE=1.01, MAE=0.801]\n",
      "Training:  20%|##        | 4/20 [00:19<01:18,  4.93s/epoch, Loss=1.39e+4, RMSE=1.01, MAE=0.801]\n",
      "Training:  25%|##5       | 5/20 [00:23<01:06,  4.42s/epoch, Loss=1.39e+4, RMSE=1.01, MAE=0.801]\n",
      "Training:  30%|###       | 6/20 [00:26<00:57,  4.11s/epoch, Loss=1.39e+4, RMSE=1.01, MAE=0.801]\n",
      "Training:  30%|###       | 6/20 [00:32<00:57,  4.11s/epoch, Loss=1.22e+4, RMSE=0.962, MAE=0.766]\n",
      "Training:  35%|###5      | 7/20 [00:32<01:00,  4.66s/epoch, Loss=1.22e+4, RMSE=0.962, MAE=0.766]\n",
      "Training:  40%|####      | 8/20 [00:35<00:51,  4.29s/epoch, Loss=1.22e+4, RMSE=0.962, MAE=0.766]\n",
      "Training:  45%|####5     | 9/20 [00:39<00:44,  4.05s/epoch, Loss=1.22e+4, RMSE=0.962, MAE=0.766]\n",
      "Training:  45%|####5     | 9/20 [00:45<00:44,  4.05s/epoch, Loss=1.09e+4, RMSE=0.941, MAE=0.745]\n",
      "Training:  50%|#####     | 10/20 [00:45<00:47,  4.77s/epoch, Loss=1.09e+4, RMSE=0.941, MAE=0.745]\n",
      "Training:  55%|#####5    | 11/20 [00:49<00:40,  4.48s/epoch, Loss=1.09e+4, RMSE=0.941, MAE=0.745]\n",
      "Training:  60%|######    | 12/20 [00:53<00:34,  4.28s/epoch, Loss=1.09e+4, RMSE=0.941, MAE=0.745]\n",
      "Training:  60%|######    | 12/20 [00:59<00:34,  4.28s/epoch, Loss=9.81e+3, RMSE=0.94, MAE=0.748] \n",
      "Training:  65%|######5   | 13/20 [00:59<00:34,  4.98s/epoch, Loss=9.81e+3, RMSE=0.94, MAE=0.748]\n",
      "Training:  70%|#######   | 14/20 [01:03<00:28,  4.67s/epoch, Loss=9.81e+3, RMSE=0.94, MAE=0.748]\n",
      "Training:  75%|#######5  | 15/20 [01:07<00:22,  4.40s/epoch, Loss=9.81e+3, RMSE=0.94, MAE=0.748]\n",
      "Training:  75%|#######5  | 15/20 [01:14<00:22,  4.40s/epoch, Loss=8.91e+3, RMSE=0.937, MAE=0.741]\n",
      "Training:  80%|########  | 16/20 [01:14<00:20,  5.08s/epoch, Loss=8.91e+3, RMSE=0.937, MAE=0.741]\n",
      "Training:  85%|########5 | 17/20 [01:17<00:13,  4.60s/epoch, Loss=8.91e+3, RMSE=0.937, MAE=0.741]\n",
      "Training:  90%|######### | 18/20 [01:21<00:08,  4.23s/epoch, Loss=8.91e+3, RMSE=0.937, MAE=0.741]\n",
      "Training:  90%|######### | 18/20 [01:27<00:08,  4.23s/epoch, Loss=8.1e+3, RMSE=0.949, MAE=0.751] \n",
      "Training:  95%|#########5| 19/20 [01:27<00:04,  4.76s/epoch, Loss=8.1e+3, RMSE=0.949, MAE=0.751]\n",
      "Training: 100%|##########| 20/20 [01:30<00:00,  4.44s/epoch, Loss=8.1e+3, RMSE=0.949, MAE=0.751]\n",
      "Training: 100%|##########| 20/20 [01:30<00:00,  4.54s/epoch, Loss=8.1e+3, RMSE=0.949, MAE=0.751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 40%|████      | 2/5 [09:07<10:36, 212.16s/trial, best loss: 0.8983660924581586]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:04<?, ?epoch/s, Loss=6.93e+4, RMSE=1.06, MAE=0.829]\n",
      "Training:   5%|5         | 1/20 [00:04<01:31,  4.82s/epoch, Loss=6.93e+4, RMSE=1.06, MAE=0.829]\n",
      "Training:  10%|#         | 2/20 [00:07<00:59,  3.31s/epoch, Loss=6.93e+4, RMSE=1.06, MAE=0.829]\n",
      "Training:  15%|#5        | 3/20 [00:09<00:48,  2.83s/epoch, Loss=6.93e+4, RMSE=1.06, MAE=0.829]\n",
      "Training:  15%|#5        | 3/20 [00:14<00:48,  2.83s/epoch, Loss=2.74e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  20%|##        | 4/20 [00:14<01:00,  3.76s/epoch, Loss=2.74e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  25%|##5       | 5/20 [00:16<00:48,  3.25s/epoch, Loss=2.74e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  30%|###       | 6/20 [00:19<00:40,  2.92s/epoch, Loss=2.74e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  30%|###       | 6/20 [00:23<00:40,  2.92s/epoch, Loss=2.71e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  35%|###5      | 7/20 [00:23<00:45,  3.48s/epoch, Loss=2.71e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  40%|####      | 8/20 [00:26<00:37,  3.09s/epoch, Loss=2.71e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  45%|####5     | 9/20 [00:28<00:30,  2.82s/epoch, Loss=2.71e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  45%|####5     | 9/20 [00:32<00:30,  2.82s/epoch, Loss=2.67e+4, RMSE=0.999, MAE=0.795]\n",
      "Training:  50%|#####     | 10/20 [00:32<00:33,  3.38s/epoch, Loss=2.67e+4, RMSE=0.999, MAE=0.795]\n",
      "Training:  55%|#####5    | 11/20 [00:35<00:27,  3.05s/epoch, Loss=2.67e+4, RMSE=0.999, MAE=0.795]\n",
      "Training:  60%|######    | 12/20 [00:37<00:22,  2.81s/epoch, Loss=2.67e+4, RMSE=0.999, MAE=0.795]\n",
      "Training:  60%|######    | 12/20 [00:42<00:22,  2.81s/epoch, Loss=2.58e+4, RMSE=0.985, MAE=0.782]\n",
      "Training:  65%|######5   | 13/20 [00:42<00:23,  3.38s/epoch, Loss=2.58e+4, RMSE=0.985, MAE=0.782]\n",
      "Training:  70%|#######   | 14/20 [00:44<00:18,  3.04s/epoch, Loss=2.58e+4, RMSE=0.985, MAE=0.782]\n",
      "Training:  75%|#######5  | 15/20 [00:46<00:14,  2.87s/epoch, Loss=2.58e+4, RMSE=0.985, MAE=0.782]\n",
      "Training:  75%|#######5  | 15/20 [00:52<00:14,  2.87s/epoch, Loss=2.46e+4, RMSE=0.97, MAE=0.772] \n",
      "Training:  80%|########  | 16/20 [00:52<00:14,  3.55s/epoch, Loss=2.46e+4, RMSE=0.97, MAE=0.772]\n",
      "Training:  85%|########5 | 17/20 [00:54<00:09,  3.25s/epoch, Loss=2.46e+4, RMSE=0.97, MAE=0.772]\n",
      "Training:  90%|######### | 18/20 [00:57<00:06,  3.04s/epoch, Loss=2.46e+4, RMSE=0.97, MAE=0.772]\n",
      "Training:  90%|######### | 18/20 [01:01<00:06,  3.04s/epoch, Loss=2.35e+4, RMSE=0.96, MAE=0.765]\n",
      "Training:  95%|#########5| 19/20 [01:01<00:03,  3.56s/epoch, Loss=2.35e+4, RMSE=0.96, MAE=0.765]\n",
      "Training: 100%|##########| 20/20 [01:04<00:00,  3.19s/epoch, Loss=2.35e+4, RMSE=0.96, MAE=0.765]\n",
      "Training: 100%|##########| 20/20 [01:04<00:00,  3.21s/epoch, Loss=2.35e+4, RMSE=0.96, MAE=0.765]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 60%|██████    | 3/5 [12:25<06:45, 202.54s/trial, best loss: 0.8983660924581586]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:08<?, ?epoch/s, Loss=4.13e+4, RMSE=1.05, MAE=0.845]\n",
      "Training:   5%|5         | 1/20 [00:08<02:44,  8.66s/epoch, Loss=4.13e+4, RMSE=1.05, MAE=0.845]\n",
      "Training:  10%|#         | 2/20 [00:14<02:03,  6.84s/epoch, Loss=4.13e+4, RMSE=1.05, MAE=0.845]\n",
      "Training:  15%|#5        | 3/20 [00:19<01:46,  6.25s/epoch, Loss=4.13e+4, RMSE=1.05, MAE=0.845]\n",
      "Training:  15%|#5        | 3/20 [00:28<01:46,  6.25s/epoch, Loss=1.36e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  20%|##        | 4/20 [00:28<01:56,  7.28s/epoch, Loss=1.36e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  25%|##5       | 5/20 [00:34<01:42,  6.85s/epoch, Loss=1.36e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  30%|###       | 6/20 [00:40<01:30,  6.45s/epoch, Loss=1.36e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  30%|###       | 6/20 [00:49<01:30,  6.45s/epoch, Loss=1.33e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  35%|###5      | 7/20 [00:49<01:36,  7.44s/epoch, Loss=1.33e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  40%|####      | 8/20 [00:56<01:25,  7.09s/epoch, Loss=1.33e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  45%|####5     | 9/20 [01:02<01:16,  6.94s/epoch, Loss=1.33e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  45%|####5     | 9/20 [01:11<01:16,  6.94s/epoch, Loss=1.31e+4, RMSE=0.987, MAE=0.787]\n",
      "Training:  50%|#####     | 10/20 [01:11<01:16,  7.60s/epoch, Loss=1.31e+4, RMSE=0.987, MAE=0.787]\n",
      "Training:  55%|#####5    | 11/20 [01:17<01:02,  6.96s/epoch, Loss=1.31e+4, RMSE=0.987, MAE=0.787]\n",
      "Training:  60%|######    | 12/20 [01:23<00:52,  6.59s/epoch, Loss=1.31e+4, RMSE=0.987, MAE=0.787]\n",
      "Training:  60%|######    | 12/20 [01:31<00:52,  6.59s/epoch, Loss=1.28e+4, RMSE=0.978, MAE=0.782]\n",
      "Training:  65%|######5   | 13/20 [01:31<00:49,  7.09s/epoch, Loss=1.28e+4, RMSE=0.978, MAE=0.782]\n",
      "Training:  70%|#######   | 14/20 [01:36<00:38,  6.49s/epoch, Loss=1.28e+4, RMSE=0.978, MAE=0.782]\n",
      "Training:  75%|#######5  | 15/20 [01:41<00:30,  6.14s/epoch, Loss=1.28e+4, RMSE=0.978, MAE=0.782]\n",
      "Training:  75%|#######5  | 15/20 [01:52<00:30,  6.14s/epoch, Loss=1.26e+4, RMSE=0.97, MAE=0.774] \n",
      "Training:  80%|########  | 16/20 [01:52<00:29,  7.40s/epoch, Loss=1.26e+4, RMSE=0.97, MAE=0.774]\n",
      "Training:  85%|########5 | 17/20 [01:59<00:21,  7.28s/epoch, Loss=1.26e+4, RMSE=0.97, MAE=0.774]\n",
      "Training:  90%|######### | 18/20 [02:05<00:14,  7.12s/epoch, Loss=1.26e+4, RMSE=0.97, MAE=0.774]\n",
      "Training:  90%|######### | 18/20 [02:15<00:14,  7.12s/epoch, Loss=1.24e+4, RMSE=0.963, MAE=0.767]\n",
      "Training:  95%|#########5| 19/20 [02:15<00:07,  7.76s/epoch, Loss=1.24e+4, RMSE=0.963, MAE=0.767]\n",
      "Training: 100%|##########| 20/20 [02:21<00:00,  7.31s/epoch, Loss=1.24e+4, RMSE=0.963, MAE=0.767]\n",
      "Training: 100%|##########| 20/20 [02:21<00:00,  7.07s/epoch, Loss=1.24e+4, RMSE=0.963, MAE=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 80%|████████  | 4/5 [17:00<03:51, 231.63s/trial, best loss: 0.8983660924581586]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:08<?, ?epoch/s, Loss=2.41e+4, RMSE=1.09, MAE=0.852]\n",
      "Training:   5%|5         | 1/20 [00:08<02:32,  8.00s/epoch, Loss=2.41e+4, RMSE=1.09, MAE=0.852]\n",
      "Training:  10%|#         | 2/20 [00:13<01:57,  6.52s/epoch, Loss=2.41e+4, RMSE=1.09, MAE=0.852]\n",
      "Training:  15%|#5        | 3/20 [00:18<01:43,  6.06s/epoch, Loss=2.41e+4, RMSE=1.09, MAE=0.852]\n",
      "Training:  15%|#5        | 3/20 [00:27<01:43,  6.06s/epoch, Loss=1.32e+4, RMSE=0.979, MAE=0.783]\n",
      "Training:  20%|##        | 4/20 [00:27<01:50,  6.91s/epoch, Loss=1.32e+4, RMSE=0.979, MAE=0.783]\n",
      "Training:  25%|##5       | 5/20 [00:32<01:36,  6.40s/epoch, Loss=1.32e+4, RMSE=0.979, MAE=0.783]\n",
      "Training:  30%|###       | 6/20 [00:38<01:27,  6.27s/epoch, Loss=1.32e+4, RMSE=0.979, MAE=0.783]\n",
      "Training:  30%|###       | 6/20 [00:47<01:27,  6.27s/epoch, Loss=1.08e+4, RMSE=0.923, MAE=0.736]\n",
      "Training:  35%|###5      | 7/20 [00:47<01:33,  7.17s/epoch, Loss=1.08e+4, RMSE=0.923, MAE=0.736]\n",
      "Training:  40%|####      | 8/20 [00:53<01:19,  6.64s/epoch, Loss=1.08e+4, RMSE=0.923, MAE=0.736]\n",
      "Training:  45%|####5     | 9/20 [00:58<01:08,  6.23s/epoch, Loss=1.08e+4, RMSE=0.923, MAE=0.736]\n",
      "Training:  45%|####5     | 9/20 [01:06<01:08,  6.23s/epoch, Loss=8.69e+3, RMSE=0.902, MAE=0.713]\n",
      "Training:  50%|#####     | 10/20 [01:06<01:06,  6.63s/epoch, Loss=8.69e+3, RMSE=0.902, MAE=0.713]\n",
      "Training:  55%|#####5    | 11/20 [01:11<00:54,  6.11s/epoch, Loss=8.69e+3, RMSE=0.902, MAE=0.713]\n",
      "Training:  60%|######    | 12/20 [01:15<00:45,  5.74s/epoch, Loss=8.69e+3, RMSE=0.902, MAE=0.713]\n",
      "Training:  60%|######    | 12/20 [01:23<00:45,  5.74s/epoch, Loss=6.86e+3, RMSE=0.912, MAE=0.722]\n",
      "Training:  65%|######5   | 13/20 [01:23<00:44,  6.31s/epoch, Loss=6.86e+3, RMSE=0.912, MAE=0.722]\n",
      "Training:  70%|#######   | 14/20 [01:28<00:35,  5.94s/epoch, Loss=6.86e+3, RMSE=0.912, MAE=0.722]\n",
      "Training:  75%|#######5  | 15/20 [01:33<00:28,  5.70s/epoch, Loss=6.86e+3, RMSE=0.912, MAE=0.722]\n",
      "Training:  75%|#######5  | 15/20 [01:41<00:28,  5.70s/epoch, Loss=5.4e+3, RMSE=0.935, MAE=0.74]  \n",
      "Training:  80%|########  | 16/20 [01:41<00:25,  6.26s/epoch, Loss=5.4e+3, RMSE=0.935, MAE=0.74]\n",
      "Training:  85%|########5 | 17/20 [01:46<00:17,  5.88s/epoch, Loss=5.4e+3, RMSE=0.935, MAE=0.74]\n",
      "Training:  90%|######### | 18/20 [01:51<00:11,  5.61s/epoch, Loss=5.4e+3, RMSE=0.935, MAE=0.74]\n",
      "Training:  90%|######### | 18/20 [01:58<00:11,  5.61s/epoch, Loss=4.4e+3, RMSE=0.969, MAE=0.768]\n",
      "Training:  95%|#########5| 19/20 [01:58<00:06,  6.16s/epoch, Loss=4.4e+3, RMSE=0.969, MAE=0.768]\n",
      "Training: 100%|##########| 20/20 [02:03<00:00,  5.79s/epoch, Loss=4.4e+3, RMSE=0.969, MAE=0.768]\n",
      "Training: 100%|##########| 20/20 [02:03<00:00,  6.19s/epoch, Loss=4.4e+3, RMSE=0.969, MAE=0.768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [19:08<00:00, 229.71s/trial, best loss: 0.8983660924581586]\n",
      "Best parameters: {'batch_size': 2, 'hidden_neuron': 2, 'learning_rate': 0.005181234156732431, 'reg_rate': 0.0033614508223386813}\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def objective(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    reg_rate = params['reg_rate']\n",
    "    hidden_neuron = params['hidden_neuron']\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        model = UAutoRec(sess, user, item, learning_rate=learning_rate, reg_rate=reg_rate, epoch=20, batch_size=batch_size, verbose=True)\n",
    "        model.build_network(hidden_neuron=hidden_neuron)\n",
    "        model.execute(train, vad, confounder_data)\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        rmse, mae = model.test(vad, confounder_data)\n",
    "        return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.0001, 0.01),\n",
    "    'reg_rate': hp.uniform('reg_rate', 0.0001, 0.01),\n",
    "    'hidden_neuron': hp.choice('hidden_neuron', [100, 250, 500]),\n",
    "    'batch_size': hp.choice('batch_size', [100, 200, 400])\n",
    "}\n",
    "\n",
    "# Run Hyperopt\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "\n",
    "print(\"Best parameters:\", best)\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': best['learning_rate'],\n",
    "    'reg_rate': best['reg_rate'],\n",
    "    'hidden_neuron': [100, 200, 500][best['hidden_neuron']],\n",
    "    'batch_size': [100, 200, 500][best['batch_size']]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data finished. Number of users: 6040 Number of items: 3706\n",
      "UAutoRec with Confounder.\n",
      "Train data processed shape: (3706, 6040)\n",
      "Confounder data shape: (3706, 6040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 80/80 [05:42<00:00,  4.28s/epoch, Loss=3.96e+4, RMSE=0.871, MAE=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE: 0.8713429436450515, Final MAE: 0.6891787481360501\n"
     ]
    }
   ],
   "source": [
    "def load_data_rating(dat, columns=[0, 1, 2], sep=\"\\t\"):\n",
    "    full, train, test = choose_data(dat, test_size= 0.1)\n",
    "\n",
    "    \n",
    "    # train, vad =  train_test_split(train_df, test_size=0.1, random_state=42)#pd.read_csv(train_file, sep=sep, header=None, names=['userId', 'itemId', 'rating'], usecols=columns, engine=\"python\")\n",
    "    \n",
    "    n_users = max(train['userId'].max(), test['userId'].max()) + 1\n",
    "    n_items = max(train['songId'].max(), test['songId'].max()) + 1\n",
    "\n",
    "    train_row = []\n",
    "    train_col = []\n",
    "    train_rating = []\n",
    "\n",
    "    for line in train.itertuples():\n",
    "        u = line[1]\n",
    "        i = line[2]\n",
    "        train_row.append(u)\n",
    "        train_col.append(i)\n",
    "        train_rating.append(line[3])\n",
    "\n",
    "    train_matrix = csr_matrix((train_rating, (train_row, train_col)), shape=(n_users, n_items))\n",
    "\n",
    "    test_row = []\n",
    "    test_col = []\n",
    "    test_rating = []\n",
    "    for line in test.itertuples():\n",
    "        u = line[1]\n",
    "        i = line[2]\n",
    "        test_row.append(u)\n",
    "        test_col.append(i)\n",
    "        test_rating.append(line[3])\n",
    "\n",
    "    test_matrix = csr_matrix((test_rating, (test_row, test_col)), shape=(n_users, n_items))\n",
    "\n",
    "    # vd_row = []\n",
    "    # vd_col = []\n",
    "    # vd_rating = []\n",
    "    # for line in vad.itertuples():\n",
    "    #     u = line[1]\n",
    "    #     i = line[2]\n",
    "    #     vd_row.append(u)\n",
    "    #     vd_col.append(i)\n",
    "    #     vd_rating.append(line[3])\n",
    "\n",
    "    # vd_matrix = csr_matrix((vd_rating, (vd_row,vd_col)), shape=(n_users, n_items))\n",
    "\n",
    "    print(\"Load data finished. Number of users:\", n_users, \"Number of items:\", n_items)\n",
    "    return train_matrix.todok(), test_matrix.todok(), n_users, n_items\n",
    "\n",
    "\n",
    "train, test, user, item = load_data_rating('ml', columns=[0, 1, 2], sep=\"\\t\")\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "CAUSEFIT_DIR = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/exposure_output/ml_exp_k_30.csv'\n",
    "\n",
    "conf_df = pd.read_csv(CAUSEFIT_DIR, header=None)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "confounder_data = conf_df.to_numpy()\n",
    "confounder_data = confounder_data.T\n",
    "\n",
    "# Train the final model on the entire training data\n",
    "with tf.compat.v1.Session(config=config) as sess:\n",
    "    final_model = UAutoRec(sess, user, item,  learning_rate=0.001, reg_rate=0.1, epoch=80, batch_size=500, verbose=True)#learning_rate=best_params['learning_rate'], reg_rate=best_params['reg_rate'], epoch =50 , batch_size=best_params['batch_size'], verbose=True)\n",
    "    final_model.build_network(hidden_neuron= 500) #best_params['hidden_neuron'])\n",
    "    final_model.execute(train, test, confounder_data)\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    final_rmse, final_mae = final_model.test(test, confounder_data)\n",
    "    print(f\"Final RMSE: {final_rmse}, Final MAE: {final_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\ana\\envs\\new\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m87439/87439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1ms/step\n",
      "(6040, 3706)\n",
      "(6040, 3706)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop, SGD\n",
    "\n",
    "# Define the function to create the model architecture\n",
    "def get_model(num_users, num_items, layers, reg_layers):\n",
    "    assert len(layers) == len(reg_layers), \"Each layer should have a corresponding regularization parameter\"\n",
    "    \n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "\n",
    "    # Using TensorFlow Keras initializers and regularizers\n",
    "    embedding_initializer = RandomNormal(mean=0.0, stddev=0.01)\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=layers[0]//2,\n",
    "                               embeddings_initializer=embedding_initializer,\n",
    "                               embeddings_regularizer=l2(reg_layers[0]),\n",
    "                               name='user_embedding')(user_input)\n",
    "    item_embedding = Embedding(input_dim=num_items, output_dim=layers[0]//2,\n",
    "                               embeddings_initializer=embedding_initializer,\n",
    "                               embeddings_regularizer=l2(reg_layers[0]),\n",
    "                               name='item_embedding')(item_input)\n",
    "\n",
    "    # Flatten the embedding output to remove the sequence dimension\n",
    "    user_latent = Flatten()(user_embedding)\n",
    "    item_latent = Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate the user and item embeddings\n",
    "    vector = Concatenate()([user_latent, item_latent])\n",
    "\n",
    "    # Add MLP layers as specified in the layers list\n",
    "    for idx, layer_size in enumerate(layers[1:], start=1):  # Start from 1 since layer[0] is for embeddings\n",
    "        vector = Dense(layer_size, activation='relu',\n",
    "                       kernel_regularizer=l2(reg_layers[idx]),\n",
    "                       name=f'layer{idx}')(vector)\n",
    "\n",
    "    # Prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)\n",
    "\n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    return model\n",
    "\n",
    "# Function to generate predictions\n",
    "def generate_predictions(model, num_users, num_items):\n",
    "    user_input = np.repeat(np.arange(num_users), num_items)\n",
    "    item_input = np.tile(np.arange(num_items), num_users)\n",
    "    predictions = model.predict([user_input, item_input], batch_size=256, verbose=1)\n",
    "    return user_input, item_input, predictions\n",
    "\n",
    "# Define parameters\n",
    "num_users = 6040  # Example: set the actual number of users from your dataset\n",
    "num_items = 3706  # Example: set the actual number of items from your dataset\n",
    "layers = [64, 32, 16, 8]  # Example: set the layers as per your trained model\n",
    "reg_layers = [0, 0, 0, 0]  # Example: set the regularization for each layer\n",
    "\n",
    "# Initialize and compile the model\n",
    "model = get_model(num_users, num_items, layers, reg_layers)\n",
    "learner = 'adam'  # Example: set the optimizer used during training\n",
    "learning_rate = 0.001  # Example: set the learning rate used during training\n",
    "\n",
    "if learner.lower() == \"adagrad\": \n",
    "    model.compile(optimizer=Adagrad(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"rmsprop\":\n",
    "    model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "elif learner.lower() == \"adam\":\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "else:\n",
    "    model.compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "\n",
    "# Load the trained weights\n",
    "model_out_file = 'C:/Users/Sten Stokroos/Desktop/Thesis2.0/zelf/neural_collaborative_filtering/Pretrain/ml-1m_MLP_64_32_16_8_1717442468.weights.h5'  # Change TIMESTAMP to the actual timestamp\n",
    "model.load_weights(model_out_file)\n",
    "\n",
    "# Generate predictions\n",
    "user_input, item_input, predictions = generate_predictions(model, num_users, num_items)\n",
    "prediction_matrix = np.zeros((num_users, num_items))\n",
    "for user, item, prediction in zip(user_input, item_input, predictions):\n",
    "    prediction_matrix[user, item] = prediction[0]  # Extract scalar from array\n",
    "\n",
    "# Save predictions to CSV\n",
    "print(prediction_matrix.shape)\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame(prediction_matrix)\n",
    "print(pred_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out_file = 'C:/Users/Sten Stokroos/Desktop/Thesis2.0/zelf/neural_collaborative_filtering/Data/predicted_scores_copy.csv'\n",
    "pred_df.to_csv(pred_out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3706, 6040)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Sten Stokroos/Desktop/Thesis2.0/zelf/neural_collaborative_filtering/Data/predicted_scores_copy.csv')\n",
    "confounder_data = df.to_numpy()\n",
    "# Transpose the array if needed\n",
    "confounder_data = confounder_data.T\n",
    "print(confounder_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

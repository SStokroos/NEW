{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed:  42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# dir_r3 = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/out'\n",
    "dir_ml = 'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/out'\n",
    "randseed = 42\n",
    "print(\"random seed: \", randseed)\n",
    "np.random.seed(randseed)\n",
    "\n",
    "def choose_data(dat, test_size, val_size):\n",
    "    if dat == 'ml2':\n",
    "        train = pd.read_csv(os.path.join(dir_ml, 'ml_train2.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "        test = pd.read_csv(os.path.join(dir_ml, 'ml_test2.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "        val = None  # Assuming no validation set for 'ml2'\n",
    "    elif dat == 'ml':\n",
    "        ml_full = pd.read_csv(os.path.join(dir_ml, 'ml-1m_full.csv'), sep=\"\\t\", header=None, names=['userId', 'songId', 'rating'], usecols=[0, 1, 2], engine=\"python\")\n",
    "\n",
    "        # Get unique user IDs\n",
    "\n",
    "        user_ids = ml_full['userId'].unique()\n",
    "        item_ids = ml_full['songId'].unique()\n",
    "\n",
    "        n_users = len(user_ids)\n",
    "        n_items = len(item_ids)\n",
    "        # Split user IDs for train, validation, and test sets\n",
    "        train, test = train_test_split(ml_full, test_size=test_size, random_state=42)\n",
    "        train, val = train_test_split(train, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    else:\n",
    "        print('Wrong data input')\n",
    "        return None, None, None\n",
    "\n",
    "    # Print the sizes of the datasets\n",
    "    print(f\"Train set size: {train.shape[0]} ratings\")\n",
    "    print(f\"Validation set size: {val.shape[0] if val is not None else 0} ratings\")\n",
    "    print(f\"Test set size: {test.shape[0]} ratings\")\n",
    "    print(ml_full.shape[0])\n",
    "    print(train.shape[0] + val.shape[0] +  test.shape[0])\n",
    "\n",
    "    return train, val, test, n_users, n_items\n",
    "\n",
    "def load_confounders(dat, k):\n",
    "    CAUSEFIT_DIR = f'C:/Users/Sten Stokroos/Desktop/NEW/zelf/Data/exposure_output/{dat}_exp_k_{k}.csv'\n",
    "    conf_df = pd.read_csv(CAUSEFIT_DIR, header=None)\n",
    "    confounder_data = conf_df.to_numpy().T\n",
    "    return confounder_data\n",
    "\n",
    "def load_data_rating(dat, columns=[0, 1, 2], sep=\"\\t\", include_validation=False, test_size=0.1, val_size=0.1):\n",
    "    train, val, test, n_users, n_items = choose_data(dat, test_size, val_size)\n",
    "\n",
    "    if train is None or test is None:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    if include_validation:\n",
    "        # Use the provided validation set\n",
    "        vad = val\n",
    "    else:\n",
    "        # Merge train and validation sets if validation is not required, and sort by userId\n",
    "        train = pd.concat([train, val]).sort_values(by='userId').reset_index(drop=True)\n",
    "        vad = None\n",
    "\n",
    "    def build_matrix(df):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        ratings = []\n",
    "        for line in df.itertuples():\n",
    "            rows.append(line[1])\n",
    "            cols.append(line[2])\n",
    "            ratings.append(line[3])\n",
    "        return csr_matrix((ratings, (rows, cols)), shape=(n_users, n_items)).todok()\n",
    "\n",
    "    train_matrix = build_matrix(train)\n",
    "    test_matrix = build_matrix(test)\n",
    "    vad_matrix = build_matrix(vad) if vad is not None else None\n",
    "\n",
    "    print(\"Load data finished. Number of users:\", n_users, \"Number of items:\", n_items)\n",
    "    return train_matrix, test_matrix, vad_matrix, n_users, n_items\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(module_name, class_name, k, dat='ml', include_validation=False, use_confounder=False, use_exposure=False, test_size=0.1, val_size=0.1, hidden_neuron=500, learning_rate=0.001, reg_rate=0.1, epoch=20, batch_size=200, verbose=False, T=1, display_step=1000, save_path=None):\n",
    "    train, test, vad, user, item = load_data_rating(dat, columns=[0, 1, 2], sep=\"\\t\", include_validation=include_validation, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    confounder_data = None\n",
    "    exposure_data = None\n",
    "\n",
    "    if use_confounder:\n",
    "        # Load confounder data\n",
    "        confounder_data = load_confounders(dat, k)\n",
    "    if use_exposure:\n",
    "        # Create exposure matrix\n",
    "        exposure_data = (train > 0).astype(np.float32).todense().T\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        module = importlib.import_module(module_name)\n",
    "        model_class = getattr(module, class_name)\n",
    "        final_model = model_class(sess, user, item, learning_rate=learning_rate, reg_rate=reg_rate, epoch=epoch, batch_size=batch_size, verbose=verbose, T=T, display_step=display_step)\n",
    "\n",
    "        final_model.build_network(hidden_neuron=hidden_neuron)\n",
    "        \n",
    "        if confounder_data is not None and exposure_data is not None:\n",
    "            final_model.execute(train, vad, confounder_data, exposure_data)\n",
    "        elif confounder_data is not None:\n",
    "            final_model.execute(train, vad, confounder_data)\n",
    "        else:\n",
    "            final_model.execute(train, vad)\n",
    "\n",
    "        if vad is not None:\n",
    "            if confounder_data is not None and exposure_data is not None:\n",
    "                rmse, mae = final_model.test(vad, confounder_data, exposure_data)\n",
    "            elif confounder_data is not None:\n",
    "                rmse, mae = final_model.test(vad, confounder_data)\n",
    "            else:\n",
    "                rmse, mae = final_model.test(vad)\n",
    "        else:\n",
    "            rmse, mae = None, None\n",
    "\n",
    "    return rmse, mae\n",
    "\n",
    "def objective_urec1conf(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    reg_rate = params['reg_rate']\n",
    "    hidden_neuron = params['hidden_neuron']\n",
    "    k = params['k']\n",
    "\n",
    "    rmse, mae = run_model('urec_1_conf', 'UAutoRec1conf', k, dat='ml', include_validation=True, use_confounder=True, use_exposure=False, hidden_neuron=hidden_neuron, learning_rate=learning_rate, reg_rate=reg_rate, batch_size=500, epoch=20)\n",
    "    \n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def objective_urec2confexp(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    reg_rate = params['reg_rate']\n",
    "    hidden_neuron = params['hidden_neuron']\n",
    "    k = params['k']\n",
    "\n",
    "    rmse, mae = run_model('urec_2_confexp', 'UAutoRec2confexp', k, dat='ml', include_validation=True, use_confounder=True, use_exposure=True, hidden_neuron=hidden_neuron, learning_rate=learning_rate, reg_rate=reg_rate, batch_size=500, epoch=20)\n",
    "    \n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800169 ratings                       \n",
      "Validation set size: 100022 ratings                  \n",
      "Test set size: 100022 ratings                        \n",
      "1000213                                              \n",
      "1000213                                              \n",
      "Load data finished. Number of users:                 \n",
      "6040                                                 \n",
      "Number of items:                                     \n",
      "3706                                                 \n",
      "UAutoRec with Confounder.                            \n",
      "Train data processed shape: (3706, 6040)             \n",
      "Confounder data shape: (3706, 6040)                  \n",
      "  0%|          | 0/5 [02:02<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:05<?, ?epoch/s, Loss=2.36e+5, RMSE=1.29, MAE=1.07]\n",
      "Training:   5%|5         | 1/20 [00:05<01:40,  5.29s/epoch, Loss=2.36e+5, RMSE=1.29, MAE=1.07]\n",
      "Training:   5%|5         | 1/20 [00:10<01:40,  5.29s/epoch, Loss=8.6e+4, RMSE=1.04, MAE=0.813]\n",
      "Training:  10%|#         | 2/20 [00:10<01:39,  5.54s/epoch, Loss=8.6e+4, RMSE=1.04, MAE=0.813]\n",
      "Training:  10%|#         | 2/20 [00:16<01:39,  5.54s/epoch, Loss=7.07e+4, RMSE=1.02, MAE=0.803]\n",
      "Training:  15%|#5        | 3/20 [00:16<01:31,  5.36s/epoch, Loss=7.07e+4, RMSE=1.02, MAE=0.803]\n",
      "Training:  15%|#5        | 3/20 [00:21<01:31,  5.36s/epoch, Loss=6.77e+4, RMSE=1.01, MAE=0.813]\n",
      "Training:  20%|##        | 4/20 [00:21<01:24,  5.26s/epoch, Loss=6.77e+4, RMSE=1.01, MAE=0.813]\n",
      "Training:  20%|##        | 4/20 [00:26<01:24,  5.26s/epoch, Loss=6.62e+4, RMSE=0.998, MAE=0.799]\n",
      "Training:  25%|##5       | 5/20 [00:26<01:18,  5.20s/epoch, Loss=6.62e+4, RMSE=0.998, MAE=0.799]\n",
      "Training:  25%|##5       | 5/20 [00:31<01:18,  5.20s/epoch, Loss=6.57e+4, RMSE=0.997, MAE=0.796]\n",
      "Training:  30%|###       | 6/20 [00:31<01:13,  5.26s/epoch, Loss=6.57e+4, RMSE=0.997, MAE=0.796]\n",
      "Training:  30%|###       | 6/20 [00:37<01:13,  5.26s/epoch, Loss=6.55e+4, RMSE=1, MAE=0.795]    \n",
      "Training:  35%|###5      | 7/20 [00:37<01:08,  5.28s/epoch, Loss=6.55e+4, RMSE=1, MAE=0.795]\n",
      "Training:  35%|###5      | 7/20 [00:42<01:08,  5.28s/epoch, Loss=6.54e+4, RMSE=0.999, MAE=0.792]\n",
      "Training:  40%|####      | 8/20 [00:42<01:03,  5.27s/epoch, Loss=6.54e+4, RMSE=0.999, MAE=0.792]\n",
      "Training:  40%|####      | 8/20 [00:48<01:03,  5.27s/epoch, Loss=6.52e+4, RMSE=0.995, MAE=0.794]\n",
      "Training:  45%|####5     | 9/20 [00:48<00:59,  5.42s/epoch, Loss=6.52e+4, RMSE=0.995, MAE=0.794]\n",
      "Training:  45%|####5     | 9/20 [00:53<00:59,  5.42s/epoch, Loss=6.48e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  50%|#####     | 10/20 [00:53<00:53,  5.38s/epoch, Loss=6.48e+4, RMSE=0.993, MAE=0.794]\n",
      "Training:  50%|#####     | 10/20 [00:58<00:53,  5.38s/epoch, Loss=6.45e+4, RMSE=0.992, MAE=0.791]\n",
      "Training:  55%|#####5    | 11/20 [00:58<00:47,  5.30s/epoch, Loss=6.45e+4, RMSE=0.992, MAE=0.791]\n",
      "Training:  55%|#####5    | 11/20 [01:03<00:47,  5.30s/epoch, Loss=6.42e+4, RMSE=0.99, MAE=0.789] \n",
      "Training:  60%|######    | 12/20 [01:03<00:41,  5.24s/epoch, Loss=6.42e+4, RMSE=0.99, MAE=0.789]\n",
      "Training:  60%|######    | 12/20 [01:08<00:41,  5.24s/epoch, Loss=6.37e+4, RMSE=0.986, MAE=0.787]\n",
      "Training:  65%|######5   | 13/20 [01:08<00:36,  5.26s/epoch, Loss=6.37e+4, RMSE=0.986, MAE=0.787]\n",
      "Training:  65%|######5   | 13/20 [01:14<00:36,  5.26s/epoch, Loss=6.32e+4, RMSE=0.982, MAE=0.784]\n",
      "Training:  70%|#######   | 14/20 [01:14<00:31,  5.23s/epoch, Loss=6.32e+4, RMSE=0.982, MAE=0.784]\n",
      "Training:  70%|#######   | 14/20 [01:19<00:31,  5.23s/epoch, Loss=6.25e+4, RMSE=0.978, MAE=0.781]\n",
      "Training:  75%|#######5  | 15/20 [01:19<00:26,  5.29s/epoch, Loss=6.25e+4, RMSE=0.978, MAE=0.781]\n",
      "Training:  75%|#######5  | 15/20 [01:24<00:26,  5.29s/epoch, Loss=6.2e+4, RMSE=0.974, MAE=0.777] \n",
      "Training:  80%|########  | 16/20 [01:24<00:21,  5.32s/epoch, Loss=6.2e+4, RMSE=0.974, MAE=0.777]\n",
      "Training:  80%|########  | 16/20 [01:29<00:21,  5.32s/epoch, Loss=6.13e+4, RMSE=0.97, MAE=0.771]\n",
      "Training:  85%|########5 | 17/20 [01:29<00:15,  5.25s/epoch, Loss=6.13e+4, RMSE=0.97, MAE=0.771]\n",
      "Training:  85%|########5 | 17/20 [01:34<00:15,  5.25s/epoch, Loss=6.05e+4, RMSE=0.967, MAE=0.773]\n",
      "Training:  90%|######### | 18/20 [01:34<00:10,  5.19s/epoch, Loss=6.05e+4, RMSE=0.967, MAE=0.773]\n",
      "Training:  90%|######### | 18/20 [01:40<00:10,  5.19s/epoch, Loss=5.98e+4, RMSE=0.961, MAE=0.765]\n",
      "Training:  95%|#########5| 19/20 [01:40<00:05,  5.16s/epoch, Loss=5.98e+4, RMSE=0.961, MAE=0.765]\n",
      "Training:  95%|#########5| 19/20 [01:45<00:05,  5.16s/epoch, Loss=5.89e+4, RMSE=0.957, MAE=0.761]\n",
      "Training: 100%|##########| 20/20 [01:45<00:00,  5.14s/epoch, Loss=5.89e+4, RMSE=0.957, MAE=0.761]\n",
      "Training: 100%|##########| 20/20 [01:45<00:00,  5.26s/epoch, Loss=5.89e+4, RMSE=0.957, MAE=0.761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800169 ratings                                                  \n",
      "Validation set size: 100022 ratings                                             \n",
      "Test set size: 100022 ratings                                                   \n",
      "1000213                                                                         \n",
      "1000213                                                                         \n",
      "Load data finished. Number of users:                                            \n",
      "6040                                                                            \n",
      "Number of items:                                                                \n",
      "3706                                                                            \n",
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 20%|██        | 1/5 [05:51<15:19, 229.82s/trial, best loss: 0.9571537004565192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:05<?, ?epoch/s, Loss=3.09e+5, RMSE=1.16, MAE=0.921]\n",
      "Training:   5%|5         | 1/20 [00:05<01:35,  5.04s/epoch, Loss=3.09e+5, RMSE=1.16, MAE=0.921]\n",
      "Training:   5%|5         | 1/20 [00:09<01:35,  5.04s/epoch, Loss=1.31e+5, RMSE=1.11, MAE=0.872]\n",
      "Training:  10%|#         | 2/20 [00:09<01:21,  4.53s/epoch, Loss=1.31e+5, RMSE=1.11, MAE=0.872]\n",
      "Training:  10%|#         | 2/20 [00:13<01:21,  4.53s/epoch, Loss=1.08e+5, RMSE=1.06, MAE=0.865]\n",
      "Training:  15%|#5        | 3/20 [00:13<01:13,  4.34s/epoch, Loss=1.08e+5, RMSE=1.06, MAE=0.865]\n",
      "Training:  15%|#5        | 3/20 [00:17<01:13,  4.34s/epoch, Loss=9.64e+4, RMSE=1.04, MAE=0.832]\n",
      "Training:  20%|##        | 4/20 [00:17<01:08,  4.29s/epoch, Loss=9.64e+4, RMSE=1.04, MAE=0.832]\n",
      "Training:  20%|##        | 4/20 [00:21<01:08,  4.29s/epoch, Loss=9.11e+4, RMSE=1.01, MAE=0.812]\n",
      "Training:  25%|##5       | 5/20 [00:21<01:03,  4.26s/epoch, Loss=9.11e+4, RMSE=1.01, MAE=0.812]\n",
      "Training:  25%|##5       | 5/20 [00:26<01:03,  4.26s/epoch, Loss=8.8e+4, RMSE=1.01, MAE=0.809] \n",
      "Training:  30%|###       | 6/20 [00:26<01:00,  4.29s/epoch, Loss=8.8e+4, RMSE=1.01, MAE=0.809]\n",
      "Training:  30%|###       | 6/20 [00:30<01:00,  4.29s/epoch, Loss=8.64e+4, RMSE=1.01, MAE=0.81]\n",
      "Training:  35%|###5      | 7/20 [00:30<00:55,  4.27s/epoch, Loss=8.64e+4, RMSE=1.01, MAE=0.81]\n",
      "Training:  35%|###5      | 7/20 [00:34<00:55,  4.27s/epoch, Loss=8.56e+4, RMSE=1.01, MAE=0.817]\n",
      "Training:  40%|####      | 8/20 [00:34<00:51,  4.28s/epoch, Loss=8.56e+4, RMSE=1.01, MAE=0.817]\n",
      "Training:  40%|####      | 8/20 [00:39<00:51,  4.28s/epoch, Loss=8.5e+4, RMSE=1.01, MAE=0.805] \n",
      "Training:  45%|####5     | 9/20 [00:39<00:49,  4.47s/epoch, Loss=8.5e+4, RMSE=1.01, MAE=0.805]\n",
      "Training:  45%|####5     | 9/20 [00:43<00:49,  4.47s/epoch, Loss=8.43e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  50%|#####     | 10/20 [00:43<00:44,  4.42s/epoch, Loss=8.43e+4, RMSE=1.01, MAE=0.804]\n",
      "Training:  50%|#####     | 10/20 [00:48<00:44,  4.42s/epoch, Loss=8.37e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  55%|#####5    | 11/20 [00:48<00:39,  4.37s/epoch, Loss=8.37e+4, RMSE=1.01, MAE=0.806]\n",
      "Training:  55%|#####5    | 11/20 [00:52<00:39,  4.37s/epoch, Loss=8.33e+4, RMSE=1, MAE=0.801]   \n",
      "Training:  60%|######    | 12/20 [00:52<00:34,  4.34s/epoch, Loss=8.33e+4, RMSE=1, MAE=0.801]\n",
      "Training:  60%|######    | 12/20 [00:56<00:34,  4.34s/epoch, Loss=8.27e+4, RMSE=1, MAE=0.802]\n",
      "Training:  65%|######5   | 13/20 [00:56<00:30,  4.31s/epoch, Loss=8.27e+4, RMSE=1, MAE=0.802]\n",
      "Training:  65%|######5   | 13/20 [01:00<00:30,  4.31s/epoch, Loss=8.27e+4, RMSE=1, MAE=0.805]\n",
      "Training:  70%|#######   | 14/20 [01:00<00:25,  4.30s/epoch, Loss=8.27e+4, RMSE=1, MAE=0.805]\n",
      "Training:  70%|#######   | 14/20 [01:05<00:25,  4.30s/epoch, Loss=8.22e+4, RMSE=1, MAE=0.798]\n",
      "Training:  75%|#######5  | 15/20 [01:05<00:21,  4.30s/epoch, Loss=8.22e+4, RMSE=1, MAE=0.798]\n",
      "Training:  75%|#######5  | 15/20 [01:09<00:21,  4.30s/epoch, Loss=8.18e+4, RMSE=1, MAE=0.804]\n",
      "Training:  80%|########  | 16/20 [01:09<00:17,  4.29s/epoch, Loss=8.18e+4, RMSE=1, MAE=0.804]\n",
      "Training:  80%|########  | 16/20 [01:14<00:17,  4.29s/epoch, Loss=8.16e+4, RMSE=1, MAE=0.802]\n",
      "Training:  85%|########5 | 17/20 [01:14<00:13,  4.40s/epoch, Loss=8.16e+4, RMSE=1, MAE=0.802]\n",
      "Training:  85%|########5 | 17/20 [01:18<00:13,  4.40s/epoch, Loss=8.14e+4, RMSE=1, MAE=0.799]\n",
      "Training:  90%|######### | 18/20 [01:18<00:08,  4.33s/epoch, Loss=8.14e+4, RMSE=1, MAE=0.799]\n",
      "Training:  90%|######### | 18/20 [01:22<00:08,  4.33s/epoch, Loss=8.13e+4, RMSE=1, MAE=0.8]  \n",
      "Training:  95%|#########5| 19/20 [01:22<00:04,  4.34s/epoch, Loss=8.13e+4, RMSE=1, MAE=0.8]\n",
      "Training:  95%|#########5| 19/20 [01:26<00:04,  4.34s/epoch, Loss=8.1e+4, RMSE=1, MAE=0.803]\n",
      "Training: 100%|##########| 20/20 [01:26<00:00,  4.27s/epoch, Loss=8.1e+4, RMSE=1, MAE=0.803]\n",
      "Training: 100%|##########| 20/20 [01:26<00:00,  4.34s/epoch, Loss=8.1e+4, RMSE=1, MAE=0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800169 ratings                                                  \n",
      "Validation set size: 100022 ratings                                             \n",
      "Test set size: 100022 ratings                                                   \n",
      "1000213                                                                         \n",
      "1000213                                                                         \n",
      "Load data finished. Number of users:                                            \n",
      "6040                                                                            \n",
      "Number of items:                                                                \n",
      "3706                                                                            \n",
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 40%|████      | 2/5 [09:23<10:56, 218.86s/trial, best loss: 0.9571537004565192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:05<?, ?epoch/s, Loss=4.78e+5, RMSE=1.24, MAE=1.01]\n",
      "Training:   5%|5         | 1/20 [00:05<01:46,  5.61s/epoch, Loss=4.78e+5, RMSE=1.24, MAE=1.01]\n",
      "Training:   5%|5         | 1/20 [00:10<01:46,  5.61s/epoch, Loss=1.97e+5, RMSE=1.12, MAE=0.911]\n",
      "Training:  10%|#         | 2/20 [00:10<01:38,  5.47s/epoch, Loss=1.97e+5, RMSE=1.12, MAE=0.911]\n",
      "Training:  10%|#         | 2/20 [00:16<01:38,  5.47s/epoch, Loss=1.38e+5, RMSE=1.07, MAE=0.856]\n",
      "Training:  15%|#5        | 3/20 [00:16<01:31,  5.38s/epoch, Loss=1.38e+5, RMSE=1.07, MAE=0.856]\n",
      "Training:  15%|#5        | 3/20 [00:21<01:31,  5.38s/epoch, Loss=1.2e+5, RMSE=1.08, MAE=0.867] \n",
      "Training:  20%|##        | 4/20 [00:21<01:26,  5.38s/epoch, Loss=1.2e+5, RMSE=1.08, MAE=0.867]\n",
      "Training:  20%|##        | 4/20 [00:27<01:26,  5.38s/epoch, Loss=1.14e+5, RMSE=1.08, MAE=0.857]\n",
      "Training:  25%|##5       | 5/20 [00:27<01:23,  5.56s/epoch, Loss=1.14e+5, RMSE=1.08, MAE=0.857]\n",
      "Training:  25%|##5       | 5/20 [00:33<01:23,  5.56s/epoch, Loss=1.13e+5, RMSE=1.08, MAE=0.864]\n",
      "Training:  30%|###       | 6/20 [00:33<01:17,  5.56s/epoch, Loss=1.13e+5, RMSE=1.08, MAE=0.864]\n",
      "Training:  30%|###       | 6/20 [00:38<01:17,  5.56s/epoch, Loss=1.12e+5, RMSE=1.07, MAE=0.857]\n",
      "Training:  35%|###5      | 7/20 [00:38<01:11,  5.52s/epoch, Loss=1.12e+5, RMSE=1.07, MAE=0.857]\n",
      "Training:  35%|###5      | 7/20 [00:43<01:11,  5.52s/epoch, Loss=1.12e+5, RMSE=1.07, MAE=0.861]\n",
      "Training:  40%|####      | 8/20 [00:43<01:05,  5.46s/epoch, Loss=1.12e+5, RMSE=1.07, MAE=0.861]\n",
      "Training:  40%|####      | 8/20 [00:49<01:05,  5.46s/epoch, Loss=1.11e+5, RMSE=1.1, MAE=0.886] \n",
      "Training:  45%|####5     | 9/20 [00:49<00:59,  5.40s/epoch, Loss=1.11e+5, RMSE=1.1, MAE=0.886]\n",
      "Training:  45%|####5     | 9/20 [00:54<00:59,  5.40s/epoch, Loss=1.11e+5, RMSE=1.07, MAE=0.857]\n",
      "Training:  50%|#####     | 10/20 [00:54<00:53,  5.38s/epoch, Loss=1.11e+5, RMSE=1.07, MAE=0.857]\n",
      "Training:  50%|#####     | 10/20 [00:59<00:53,  5.38s/epoch, Loss=1.1e+5, RMSE=1.07, MAE=0.862] \n",
      "Training:  55%|#####5    | 11/20 [00:59<00:48,  5.37s/epoch, Loss=1.1e+5, RMSE=1.07, MAE=0.862]\n",
      "Training:  55%|#####5    | 11/20 [01:05<00:48,  5.37s/epoch, Loss=1.1e+5, RMSE=1.08, MAE=0.872]\n",
      "Training:  60%|######    | 12/20 [01:05<00:43,  5.44s/epoch, Loss=1.1e+5, RMSE=1.08, MAE=0.872]\n",
      "Training:  60%|######    | 12/20 [01:10<00:43,  5.44s/epoch, Loss=1.1e+5, RMSE=1.08, MAE=0.866]\n",
      "Training:  65%|######5   | 13/20 [01:10<00:37,  5.38s/epoch, Loss=1.1e+5, RMSE=1.08, MAE=0.866]\n",
      "Training:  65%|######5   | 13/20 [01:16<00:37,  5.38s/epoch, Loss=1.09e+5, RMSE=1.09, MAE=0.87]\n",
      "Training:  70%|#######   | 14/20 [01:16<00:32,  5.39s/epoch, Loss=1.09e+5, RMSE=1.09, MAE=0.87]\n",
      "Training:  70%|#######   | 14/20 [01:21<00:32,  5.39s/epoch, Loss=1.08e+5, RMSE=1.09, MAE=0.879]\n",
      "Training:  75%|#######5  | 15/20 [01:21<00:27,  5.47s/epoch, Loss=1.08e+5, RMSE=1.09, MAE=0.879]\n",
      "Training:  75%|#######5  | 15/20 [01:27<00:27,  5.47s/epoch, Loss=1.09e+5, RMSE=1.1, MAE=0.895] \n",
      "Training:  80%|########  | 16/20 [01:27<00:22,  5.53s/epoch, Loss=1.09e+5, RMSE=1.1, MAE=0.895]\n",
      "Training:  80%|########  | 16/20 [01:33<00:22,  5.53s/epoch, Loss=1.08e+5, RMSE=1.08, MAE=0.871]\n",
      "Training:  85%|########5 | 17/20 [01:33<00:17,  5.68s/epoch, Loss=1.08e+5, RMSE=1.08, MAE=0.871]\n",
      "Training:  85%|########5 | 17/20 [01:38<00:17,  5.68s/epoch, Loss=1.07e+5, RMSE=1.07, MAE=0.848]\n",
      "Training:  90%|######### | 18/20 [01:38<00:11,  5.65s/epoch, Loss=1.07e+5, RMSE=1.07, MAE=0.848]\n",
      "Training:  90%|######### | 18/20 [01:44<00:11,  5.65s/epoch, Loss=1.06e+5, RMSE=1.07, MAE=0.854]\n",
      "Training:  95%|#########5| 19/20 [01:44<00:05,  5.57s/epoch, Loss=1.06e+5, RMSE=1.07, MAE=0.854]\n",
      "Training:  95%|#########5| 19/20 [01:49<00:05,  5.57s/epoch, Loss=1.06e+5, RMSE=1.06, MAE=0.847]\n",
      "Training: 100%|##########| 20/20 [01:49<00:00,  5.53s/epoch, Loss=1.06e+5, RMSE=1.06, MAE=0.847]\n",
      "Training: 100%|##########| 20/20 [01:49<00:00,  5.49s/epoch, Loss=1.06e+5, RMSE=1.06, MAE=0.847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800169 ratings                                                  \n",
      "Validation set size: 100022 ratings                                             \n",
      "Test set size: 100022 ratings                                                   \n",
      "1000213                                                                         \n",
      "1000213                                                                         \n",
      "Load data finished. Number of users:                                            \n",
      "6040                                                                            \n",
      "Number of items:                                                                \n",
      "3706                                                                            \n",
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 60%|██████    | 3/5 [13:17<07:32, 226.45s/trial, best loss: 0.9571537004565192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:05<?, ?epoch/s, Loss=2.75e+5, RMSE=1.31, MAE=1.08]\n",
      "Training:   5%|5         | 1/20 [00:05<01:49,  5.76s/epoch, Loss=2.75e+5, RMSE=1.31, MAE=1.08]\n",
      "Training:   5%|5         | 1/20 [00:11<01:49,  5.76s/epoch, Loss=1.25e+5, RMSE=1.03, MAE=0.808]\n",
      "Training:  10%|#         | 2/20 [00:11<01:39,  5.54s/epoch, Loss=1.25e+5, RMSE=1.03, MAE=0.808]\n",
      "Training:  10%|#         | 2/20 [00:16<01:39,  5.54s/epoch, Loss=9.95e+4, RMSE=1.02, MAE=0.803]\n",
      "Training:  15%|#5        | 3/20 [00:16<01:32,  5.43s/epoch, Loss=9.95e+4, RMSE=1.02, MAE=0.803]\n",
      "Training:  15%|#5        | 3/20 [00:21<01:32,  5.43s/epoch, Loss=8.74e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  20%|##        | 4/20 [00:21<01:24,  5.29s/epoch, Loss=8.74e+4, RMSE=1.01, MAE=0.799]\n",
      "Training:  20%|##        | 4/20 [00:26<01:24,  5.29s/epoch, Loss=8.14e+4, RMSE=1, MAE=0.797]   \n",
      "Training:  25%|##5       | 5/20 [00:26<01:19,  5.27s/epoch, Loss=8.14e+4, RMSE=1, MAE=0.797]\n",
      "Training:  25%|##5       | 5/20 [00:31<01:19,  5.27s/epoch, Loss=7.85e+4, RMSE=1, MAE=0.799]\n",
      "Training:  30%|###       | 6/20 [00:31<01:13,  5.23s/epoch, Loss=7.85e+4, RMSE=1, MAE=0.799]\n",
      "Training:  30%|###       | 6/20 [00:37<01:13,  5.23s/epoch, Loss=7.69e+4, RMSE=0.997, MAE=0.792]\n",
      "Training:  35%|###5      | 7/20 [00:37<01:07,  5.20s/epoch, Loss=7.69e+4, RMSE=0.997, MAE=0.792]\n",
      "Training:  35%|###5      | 7/20 [00:42<01:07,  5.20s/epoch, Loss=7.62e+4, RMSE=0.992, MAE=0.793]\n",
      "Training:  40%|####      | 8/20 [00:42<01:04,  5.38s/epoch, Loss=7.62e+4, RMSE=0.992, MAE=0.793]\n",
      "Training:  40%|####      | 8/20 [00:48<01:04,  5.38s/epoch, Loss=7.55e+4, RMSE=0.994, MAE=0.797]\n",
      "Training:  45%|####5     | 9/20 [00:48<00:58,  5.34s/epoch, Loss=7.55e+4, RMSE=0.994, MAE=0.797]\n",
      "Training:  45%|####5     | 9/20 [00:53<00:58,  5.34s/epoch, Loss=7.49e+4, RMSE=0.984, MAE=0.786]\n",
      "Training:  50%|#####     | 10/20 [00:53<00:52,  5.24s/epoch, Loss=7.49e+4, RMSE=0.984, MAE=0.786]\n",
      "Training:  50%|#####     | 10/20 [00:58<00:52,  5.24s/epoch, Loss=7.49e+4, RMSE=0.987, MAE=0.781]\n",
      "Training:  55%|#####5    | 11/20 [00:58<00:46,  5.20s/epoch, Loss=7.49e+4, RMSE=0.987, MAE=0.781]\n",
      "Training:  55%|#####5    | 11/20 [01:03<00:46,  5.20s/epoch, Loss=7.54e+4, RMSE=0.987, MAE=0.783]\n",
      "Training:  60%|######    | 12/20 [01:03<00:42,  5.35s/epoch, Loss=7.54e+4, RMSE=0.987, MAE=0.783]\n",
      "Training:  60%|######    | 12/20 [01:09<00:42,  5.35s/epoch, Loss=7.47e+4, RMSE=0.987, MAE=0.782]\n",
      "Training:  65%|######5   | 13/20 [01:09<00:37,  5.32s/epoch, Loss=7.47e+4, RMSE=0.987, MAE=0.782]\n",
      "Training:  65%|######5   | 13/20 [01:14<00:37,  5.32s/epoch, Loss=7.51e+4, RMSE=0.988, MAE=0.789]\n",
      "Training:  70%|#######   | 14/20 [01:14<00:32,  5.37s/epoch, Loss=7.51e+4, RMSE=0.988, MAE=0.789]\n",
      "Training:  70%|#######   | 14/20 [01:19<00:32,  5.37s/epoch, Loss=7.43e+4, RMSE=0.978, MAE=0.783]\n",
      "Training:  75%|#######5  | 15/20 [01:19<00:26,  5.36s/epoch, Loss=7.43e+4, RMSE=0.978, MAE=0.783]\n",
      "Training:  75%|#######5  | 15/20 [01:25<00:26,  5.36s/epoch, Loss=7.37e+4, RMSE=0.977, MAE=0.773]\n",
      "Training:  80%|########  | 16/20 [01:25<00:21,  5.46s/epoch, Loss=7.37e+4, RMSE=0.977, MAE=0.773]\n",
      "Training:  80%|########  | 16/20 [01:31<00:21,  5.46s/epoch, Loss=7.39e+4, RMSE=0.974, MAE=0.774]\n",
      "Training:  85%|########5 | 17/20 [01:31<00:16,  5.63s/epoch, Loss=7.39e+4, RMSE=0.974, MAE=0.774]\n",
      "Training:  85%|########5 | 17/20 [01:37<00:16,  5.63s/epoch, Loss=7.38e+4, RMSE=0.983, MAE=0.79] \n",
      "Training:  90%|######### | 18/20 [01:37<00:11,  5.72s/epoch, Loss=7.38e+4, RMSE=0.983, MAE=0.79]\n",
      "Training:  90%|######### | 18/20 [01:42<00:11,  5.72s/epoch, Loss=7.45e+4, RMSE=0.99, MAE=0.78] \n",
      "Training:  95%|#########5| 19/20 [01:42<00:05,  5.60s/epoch, Loss=7.45e+4, RMSE=0.99, MAE=0.78]\n",
      "Training:  95%|#########5| 19/20 [01:48<00:05,  5.60s/epoch, Loss=7.46e+4, RMSE=0.976, MAE=0.775]\n",
      "Training: 100%|##########| 20/20 [01:48<00:00,  5.54s/epoch, Loss=7.46e+4, RMSE=0.976, MAE=0.775]\n",
      "Training: 100%|##########| 20/20 [01:48<00:00,  5.42s/epoch, Loss=7.46e+4, RMSE=0.976, MAE=0.775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 800169 ratings                                                  \n",
      "Validation set size: 100022 ratings                                             \n",
      "Test set size: 100022 ratings                                                   \n",
      "1000213                                                                         \n",
      "1000213                                                                         \n",
      "Load data finished. Number of users:                                            \n",
      "6040                                                                            \n",
      "Number of items:                                                                \n",
      "3706                                                                            \n",
      "UAutoRec with Confounder.                                                       \n",
      "Train data processed shape: (3706, 6040)                                        \n",
      "Confounder data shape: (3706, 6040)                                             \n",
      " 80%|████████  | 4/5 [17:25<03:48, 228.82s/trial, best loss: 0.9571537004565192]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20 [00:00<?, ?epoch/s]\n",
      "Training:   0%|          | 0/20 [00:05<?, ?epoch/s, Loss=2.41e+5, RMSE=1.24, MAE=1.03]\n",
      "Training:   5%|5         | 1/20 [00:05<01:52,  5.94s/epoch, Loss=2.41e+5, RMSE=1.24, MAE=1.03]\n",
      "Training:   5%|5         | 1/20 [00:11<01:52,  5.94s/epoch, Loss=7.87e+4, RMSE=1.04, MAE=0.84]\n",
      "Training:  10%|#         | 2/20 [00:11<01:46,  5.92s/epoch, Loss=7.87e+4, RMSE=1.04, MAE=0.84]\n",
      "Training:  10%|#         | 2/20 [00:17<01:46,  5.92s/epoch, Loss=6.99e+4, RMSE=1.01, MAE=0.797]\n",
      "Training:  15%|#5        | 3/20 [00:17<01:38,  5.81s/epoch, Loss=6.99e+4, RMSE=1.01, MAE=0.797]\n",
      "Training:  15%|#5        | 3/20 [00:23<01:38,  5.81s/epoch, Loss=6.74e+4, RMSE=1.01, MAE=0.796]\n",
      "Training:  20%|##        | 4/20 [00:23<01:33,  5.84s/epoch, Loss=6.74e+4, RMSE=1.01, MAE=0.796]\n",
      "Training:  20%|##        | 4/20 [00:29<01:33,  5.84s/epoch, Loss=6.65e+4, RMSE=1, MAE=0.796]   \n",
      "Training:  25%|##5       | 5/20 [00:29<01:29,  5.98s/epoch, Loss=6.65e+4, RMSE=1, MAE=0.796]\n",
      "Training:  25%|##5       | 5/20 [00:34<01:29,  5.98s/epoch, Loss=6.62e+4, RMSE=1, MAE=0.8]  \n",
      "Training:  30%|###       | 6/20 [00:34<01:19,  5.67s/epoch, Loss=6.62e+4, RMSE=1, MAE=0.8]\n",
      "Training:  30%|###       | 6/20 [00:39<01:19,  5.67s/epoch, Loss=6.62e+4, RMSE=1, MAE=0.797]\n",
      "Training:  35%|###5      | 7/20 [00:39<01:10,  5.45s/epoch, Loss=6.62e+4, RMSE=1, MAE=0.797]\n",
      "Training:  35%|###5      | 7/20 [00:45<01:10,  5.45s/epoch, Loss=6.6e+4, RMSE=1, MAE=0.797] \n",
      "Training:  40%|####      | 8/20 [00:45<01:08,  5.71s/epoch, Loss=6.6e+4, RMSE=1, MAE=0.797]\n",
      "Training:  40%|####      | 8/20 [00:51<01:08,  5.71s/epoch, Loss=6.59e+4, RMSE=1, MAE=0.799]\n",
      "Training:  45%|####5     | 9/20 [00:51<01:01,  5.56s/epoch, Loss=6.59e+4, RMSE=1, MAE=0.799]\n",
      "Training:  45%|####5     | 9/20 [00:56<01:01,  5.56s/epoch, Loss=6.56e+4, RMSE=0.998, MAE=0.799]\n",
      "Training:  50%|#####     | 10/20 [00:56<00:54,  5.49s/epoch, Loss=6.56e+4, RMSE=0.998, MAE=0.799]\n",
      "Training:  50%|#####     | 10/20 [01:01<00:54,  5.49s/epoch, Loss=6.5e+4, RMSE=0.996, MAE=0.793] \n",
      "Training:  55%|#####5    | 11/20 [01:01<00:49,  5.46s/epoch, Loss=6.5e+4, RMSE=0.996, MAE=0.793]\n",
      "Training:  55%|#####5    | 11/20 [01:07<00:49,  5.46s/epoch, Loss=6.44e+4, RMSE=0.991, MAE=0.787]\n",
      "Training:  60%|######    | 12/20 [01:07<00:43,  5.40s/epoch, Loss=6.44e+4, RMSE=0.991, MAE=0.787]\n",
      "Training:  60%|######    | 12/20 [01:11<00:43,  5.40s/epoch, Loss=6.4e+4, RMSE=0.986, MAE=0.783] \n",
      "Training:  65%|######5   | 13/20 [01:11<00:36,  5.22s/epoch, Loss=6.4e+4, RMSE=0.986, MAE=0.783]\n",
      "Training:  65%|######5   | 13/20 [01:16<00:36,  5.22s/epoch, Loss=6.34e+4, RMSE=0.984, MAE=0.784]\n",
      "Training:  70%|#######   | 14/20 [01:16<00:30,  5.02s/epoch, Loss=6.34e+4, RMSE=0.984, MAE=0.784]\n",
      "Training:  70%|#######   | 14/20 [01:21<00:30,  5.02s/epoch, Loss=6.28e+4, RMSE=0.984, MAE=0.781]\n",
      "Training:  75%|#######5  | 15/20 [01:21<00:25,  5.06s/epoch, Loss=6.28e+4, RMSE=0.984, MAE=0.781]\n",
      "Training:  75%|#######5  | 15/20 [01:26<00:25,  5.06s/epoch, Loss=6.25e+4, RMSE=0.979, MAE=0.775]\n",
      "Training:  80%|########  | 16/20 [01:26<00:20,  5.09s/epoch, Loss=6.25e+4, RMSE=0.979, MAE=0.775]\n",
      "Training:  80%|########  | 16/20 [01:31<00:20,  5.09s/epoch, Loss=6.19e+4, RMSE=0.976, MAE=0.777]\n",
      "Training:  85%|########5 | 17/20 [01:31<00:14,  4.96s/epoch, Loss=6.19e+4, RMSE=0.976, MAE=0.777]\n",
      "Training:  85%|########5 | 17/20 [01:36<00:14,  4.96s/epoch, Loss=6.11e+4, RMSE=0.969, MAE=0.772]\n",
      "Training:  90%|######### | 18/20 [01:36<00:09,  4.87s/epoch, Loss=6.11e+4, RMSE=0.969, MAE=0.772]\n",
      "Training:  90%|######### | 18/20 [01:40<00:09,  4.87s/epoch, Loss=6.06e+4, RMSE=0.968, MAE=0.771]\n",
      "Training:  95%|#########5| 19/20 [01:40<00:04,  4.85s/epoch, Loss=6.06e+4, RMSE=0.968, MAE=0.771]\n",
      "Training:  95%|#########5| 19/20 [01:45<00:04,  4.85s/epoch, Loss=5.98e+4, RMSE=0.966, MAE=0.767]\n",
      "Training: 100%|##########| 20/20 [01:45<00:00,  4.78s/epoch, Loss=5.98e+4, RMSE=0.966, MAE=0.767]\n",
      "Training: 100%|##########| 20/20 [01:45<00:00,  5.28s/epoch, Loss=5.98e+4, RMSE=0.966, MAE=0.767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [19:14<00:00, 230.93s/trial, best loss: 0.9571537004565192]\n",
      "Best parameters for UAutoRec1conf: {'hidden_neuron': 7, 'k': 7, 'learning_rate': 0.0026072943828274455, 'reg_rate': 0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19884\\1497627185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m best_params_urec1conf = {\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;34m'reg_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reg_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;34m'hidden_neuron'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hidden_neuron'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best' is not defined"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.0001, 0.01),\n",
    "    'reg_rate': hp.choice('reg_rate', [0.001, 0.01, 0.1, 1, 100, 1000]),\n",
    "    'hidden_neuron': hp.choice('hidden_neuron', [10, 20, 40, 80, 100, 200, 300, 400, 500]),\n",
    "    'k': hp.choice('k', [1, 2, 5, 10, 20, 32, 50, 100]),\n",
    "}\n",
    "\n",
    "trials_urec1conf = Trials()\n",
    "best_urec1conf = fmin(fn=objective_urec1conf, space=space, algo=tpe.suggest, max_evals=5, trials=trials_urec1conf)\n",
    "\n",
    "print(\"Best parameters for UAutoRec1conf:\", best_urec1conf)\n",
    "\n",
    "best_params_urec1conf = {\n",
    "    'learning_rate': best_urec1conf['learning_rate'],\n",
    "    'reg_rate': [0.001, 0.01, 0.1, 1, 100, 1000][best_urec1conf['reg_rate']],\n",
    "    'hidden_neuron': [10, 20, 40, 80, 100, 200, 300, 400, 500][best_urec1conf['hidden_neuron']],\n",
    "    'k': [1, 2, 5, 10, 20, 32, 50, 100][best_urec1conf['k']]\n",
    "}\n",
    "\n",
    "print(\"Best parameters for UAutoRec1conf in detailed form:\", best_params_urec1conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for UAutoRec1conf in detailed form: {'learning_rate': 0.0026072943828274455, 'reg_rate': 0.001, 'hidden_neuron': 400, 'k': 100}\n",
      "Train set size: 800169 ratings\n",
      "Validation set size: 100022 ratings\n",
      "Test set size: 100022 ratings\n",
      "1000213\n",
      "1000213\n",
      "Load data finished. Number of users: 6040 Number of items: 3706\n",
      "UAutoRec with Confounder.\n",
      "Train data processed shape: (3706, 6040)\n",
      "Confounder data shape: (3706, 6040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 67/80 [06:10<01:11,  5.53s/epoch, Loss=2.91e+4, RMSE=0.885, MAE=0.698]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19884\\1482662463.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best parameters for UAutoRec1conf in detailed form:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params_urec1conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mfinal_rmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_mae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urec_1_conf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UAutoRec1conf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params_urec1conf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'k'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ml'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_validation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_confounder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_exposure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_neuron\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_params_urec1conf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hidden_neuron'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_params_urec1conf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_params_urec1conf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reg_rate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Final RMSE for UAutoRec1conf: {final_rmse}, Final MAE for UAutoRec1conf: {final_mae}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19884\\1482662463.py\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(module_name, class_name, k, dat, include_validation, use_confounder, use_exposure, test_size, val_size, hidden_neuron, learning_rate, reg_rate, epoch, batch_size, verbose, T, display_step, save_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfounder_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexposure_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mconfounder_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfounder_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mfinal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sten Stokroos\\Desktop\\NEW\\zelf\\urec_1_conf.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, train_data, test_data, confounder_data)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Training\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"epoch\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m                 \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfounder_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                     \u001b[0mrmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfounder_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sten Stokroos\\Desktop\\NEW\\zelf\\urec_1_conf.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, confounder_data)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 _, loss = self.sess.run([self.optimizer, self.loss],\n\u001b[1;32m---> 66\u001b[1;33m                                         feed_dict={self.rating_matrix: self.train_data[:, batch_set_idx],\n\u001b[0m\u001b[0;32m     67\u001b[0m                                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrating_matrix_mask\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_set_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                                                    self.confounder_matrix: confounder_data[:, batch_set_idx]})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_model(module_name, class_name, k, dat='ml', include_validation=False, use_confounder=False, use_exposure=False, test_size=0.1, val_size=0.1, hidden_neuron=500, learning_rate=0.001, reg_rate=0.1, epoch=20, batch_size=200, verbose=False, T=1, display_step=1000, save_path=None):\n",
    "    train, test, vad, user, item = load_data_rating(dat, columns=[0, 1, 2], sep=\"\\t\", include_validation=include_validation, test_size=test_size, val_size=val_size)\n",
    "\n",
    "    confounder_data = None\n",
    "    exposure_data = None\n",
    "\n",
    "    if use_confounder:\n",
    "        # Load confounder data\n",
    "        confounder_data = load_confounders(dat, k)\n",
    "    if use_exposure:\n",
    "        # Create exposure matrix\n",
    "        exposure_data = (train > 0).astype(np.float32).todense().T\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        module = importlib.import_module(module_name)\n",
    "        model_class = getattr(module, class_name)\n",
    "        final_model = model_class(sess, user, item, learning_rate=learning_rate, reg_rate=reg_rate, epoch=epoch, batch_size=batch_size, verbose=verbose, T=T, display_step=display_step)\n",
    "\n",
    "        final_model.build_network(hidden_neuron=hidden_neuron)\n",
    "        \n",
    "        if confounder_data is not None and exposure_data is not None:\n",
    "            final_model.execute(train, test, confounder_data, exposure_data)\n",
    "        elif confounder_data is not None:\n",
    "            final_model.execute(train, test, confounder_data)\n",
    "        else:\n",
    "            final_model.execute(train, test)\n",
    "\n",
    "        if vad is not None:\n",
    "            if confounder_data is not None and exposure_data is not None:\n",
    "                rmse, mae = final_model.test(test, confounder_data, exposure_data)\n",
    "            elif confounder_data is not None:\n",
    "                rmse, mae = final_model.test(test, confounder_data)\n",
    "            else:\n",
    "                rmse, mae = final_model.test(test)\n",
    "        else:\n",
    "            rmse, mae = None, None\n",
    "\n",
    "    return rmse, mae\n",
    "\n",
    "best_params_urec1conf = {\n",
    "    'learning_rate': best_urec1conf['learning_rate'],\n",
    "    'reg_rate': [0.001, 0.01, 0.1, 1, 100, 1000][best_urec1conf['reg_rate']],\n",
    "    'hidden_neuron': [10, 20, 40, 80, 100, 200, 300, 400, 500][best_urec1conf['hidden_neuron']],\n",
    "    'k': [1, 2, 5, 10, 20, 32, 50, 100][best_urec1conf['k']]\n",
    "}\n",
    "\n",
    "print(\"Best parameters for UAutoRec1conf in detailed form:\", best_params_urec1conf)\n",
    "\n",
    "final_rmse, final_mae = run_model('urec_1_conf', 'UAutoRec1conf', best_params_urec1conf['k'], dat='ml', include_validation=False, use_confounder=True, use_exposure=False, hidden_neuron=best_params_urec1conf['hidden_neuron'], learning_rate=best_params_urec1conf['learning_rate'], reg_rate=best_params_urec1conf['reg_rate'], batch_size=500, epoch=80)\n",
    "\n",
    "print(f\"Final RMSE for UAutoRec1conf: {final_rmse}, Final MAE for UAutoRec1conf: {final_mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0l949, 0.946, 0.94, 0,937 33, 28, 25, 18, 14, 13, 07, 03, 0, 97, 94, 93, 9, 87, 88, 886, 883, 882, 88, 79, 78, 77, 76, 75, 74, 76, 78, 79"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
